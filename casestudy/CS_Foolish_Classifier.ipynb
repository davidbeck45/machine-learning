{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSCI 470 Activities and Case Studies\n",
    "\n",
    "1. For all activities, you are allowed to collaborate with a partner. \n",
    "1. For case studies, you should work individually and are **not** allowed to collaborate.\n",
    "\n",
    "By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some considerations with regard to how these notebooks will be graded:\n",
    "\n",
    "1. Cells in which \"# YOUR CODE HERE\" is found are the cells where your graded code should be written.\n",
    "2. In order to test out or debug your code you may also create notebook cells or edit existing notebook cells other than \"# YOUR CODE HERE\". We actually highly recommend you do so to gain a better understanding of what is happening. However, during grading, **these changes are ignored**. \n",
    "2. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n",
    "3. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will not receive points for your work in that section.\n",
    "4. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n",
    "5. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the \"assert\" statements will revert to the original. Make sure you don't edit the assert statements.\n",
    "6. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n",
    "7. When you are asked to define a function, make sure you **don't** use any variables outside of the parameters passed to the function. You can think of the parameters being passed to the function as a hint. Make sure you're using all of those variables.\n",
    "8. Finally, **make sure you run \"Kernel > Restart and Run All\"** and pass all the asserts before submitting. If you don't restart the kernel, there may be some code that you ran and deleted that is still being used and that was why your asserts were passing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study: Bayesian classifier\n",
    "\n",
    "In this two-part case study you will:\n",
    "1. Build, train, and test a Bayesian classifier. This will increase your understanding of parametric models for supervised learning.\n",
    "2. Implement a Bayesian classifier model as Python class, in a manner similar to those of scikit-learn. This will improve you understanding of what goes on under the hood of scikit-learn models.\n",
    "3. Evaluate performance of your Bayesian classifer.\n",
    "4. Repeat this process, but build a Foolish classifer rather than a Bayesian classifier, then compare performance of a Foolish classifier to a Bayesian classifier. This will alert you to the sometimes misleadingly high performance scores a model might give, even when it has not learned any relationship between features and targets.\n",
    "\n",
    "## Part 2 - Comparing a \"Foolish\" classifier to a Bayesian classifier\n",
    "\n",
    "### In this notebook you will build a foolish classifier--one which only learns from the targets/labels, but not the from features. You'll then compare evaluation results with those of a scikit-learn Naive Bayesian classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading\n",
    "\n",
    "50 points total\n",
    "\n",
    "1. 10 pts - `fit` method\n",
    "2. 10 pts - `predict` and `fit_predict` methods\n",
    "3. 5 pts - Training models and making test set predictions for 4 classifiers\n",
    "4. 5 pts - `my_accuracy_score` function\n",
    "5. 15 pts - `my_confusion_matrix` function\n",
    "6. 5 pts - Using scikit-learn to get F-scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian Naive Bayes classifer\n",
    "\n",
    "In Part 1 of this study, you created a Bayes classifier that modeled the distributions of features as 2D Gaussian distributions, but ones constrained to having a diagonal covariance matrix--that is, Gaussian shapes like in the image below, having no \"diagonal\" aspect to the elliptical shape of the distribution.\n",
    "\n",
    "| ![A 2D Gaussian distribution with diagonal covariance matrix, fitted to noisy data.](./2d_noisy_gaussian.png) |\n",
    "|:--:|\n",
    "| A 2D Gaussian distribution with diagonal covariance matrix, fitted to noisy data. |\n",
    "\n",
    "Hopefully you learned a lot by taking on that challenging task! In this notebook you'll train a much dumber, Foolish Classifier, and while it'd be nice to compare its results to those of your Bayes Classifier, we want to be 100% certain that the Bayes Classifier is implemented without error, so we'll use scikit-learn's Gaussian Naive Bayes classifier instead. Performance of the Naive Bayes classifier (for the feature distributions we're assuming and using) is nearly as good as that of our Bayes classifier.\n",
    "\n",
    "Recall that a Naive Bayes classifier assumes that the individual features of our multivariate feature distribution are independent.\n",
    "\n",
    "$$P(x_1,x_2|y)=P(x_1|y)P(x_2|y)$$\n",
    "\n",
    "In Part 1, our $P(x_1,x_2|y)$ was:\n",
    "\n",
    "$$ P(x_1,x_2|y)=\\frac{ exp\\Big(-(\\frac{{(x_1-\\mu_{y,1})}^2}{\\sigma_{y,1}^2} + \\frac{{(x_2-\\mu_{y,2})}^2}{\\sigma_{y,2}^2}) /2\\Big) }{2\\pi\\sigma_{y,1}\\sigma_{y,2}}  $$\n",
    "\n",
    "But under the Naive Bayes (false) independence assumption, the probability equation becomes:\n",
    "\n",
    "$$ P(x_1,x_2|y)=P(x_1|y)P(x_2|y)=\\frac{ exp\\Big(-\\frac{{(x_1-\\mu_{y,1})}^2}{\\sigma_{y,1}^2}\\Big)exp\\Big(-\\frac{{(x_2-\\mu_{y,2})}^2}{\\sigma_{y,2}^2}\\Big)}{2\\pi\\sigma_{y,1}\\sigma_{y,2}} $$\n",
    "\n",
    "If we were to plot this distribution, you'd see one that has a slightly more rectangular shape, when compared to the elliptical shape of the figure above.\n",
    "\n",
    "The feature independence assumption makes it much easier to estimate the parameters of the probability distribution even when there are many features and limited data. After your experience in Part 1, you could likely implement the naive distribution quite easily, with only minor changes to your BayesClassifier code, __but we'll use the scikit-learn Gaussian Naive Bayes classifier to ensure that it's correct__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Foolish classifier\n",
    "\n",
    "In this notebook you'll implement a foolish classifier. It's foolish because although the user passes both features and labels (targets) to it for training, the classifier completely ignores the features. In effect, it only estimates the prior distribution of the classes, $P(y)$, and even that it sometimes does poorly.\n",
    "\n",
    "__Why are we having you do this?__ As you'll see from the evaluation scores of the Foolish classifier and the Naive Bayes classifier, some evaluation metrics can give surprisingly high scores for such a foolish approach. If you use just one or two evaluation metrics alone, you might be deceived into thinking that the foolish classifier is quite wise! But in fact, you'll know that it's really just \"guessing\" as to what the true labels are, not predicting based upon informative features. If you were to employ such a classifier, it'd eventually be discovered and hopefully you'd be fired. :)\n",
    "\n",
    "You'll implement a foolish classifier that has __three levels of foolishness__, one of which can be selected by the user when instantiating the class object:\n",
    "1. __Uniform__: P(y) is a uniform distribution. That is, aside from knowing the number of classes, it doesn't even learn from the targets themselves. It just assumes that all classes are equally likely.\n",
    "2. __Mode__: P(y) has probability of 1 for the class that occurs most often in the training labels, and 0 for all other classes. It predicts the \"mode\" of the training labels--the prediction never changes, it is always the same class.\n",
    "3. __Proportional__: P(y) probabilities are proportional to the frequency of labels in the training data. This is exactly how you were instructed to estimate the priors of the Bayes Classifier of Part 1. For example, if classes 0, 1, and 2, have 5, 10, and 35 samples, respectively, then the P(y) foolish classifier's priors for those classes will be 5/50, 10/50, and 35/50 (0.1, 0.2, and 0.7).\n",
    "\n",
    "They're all foolish, but __which do you think will score most highly__ on evaluation metrics applied to the test set? Or to the training set, for that matter, since the classifier doesn't even use the features of the training set for any learning. __Ponder this for a moment before you dive in!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model's Python class\n",
    "\n",
    "As in Part 1, you will create a Python class from scratch (aside from the skeleton we have provided) that will implement three methods that are part of most scikit-learn model classes:\n",
    "\n",
    "`fit(X, y)` - Trains the model using features, X, and labels, y, and stores the learned (fitted) model parameters as class object attributes. __Except in this case the foolish classifier will take in feature and labels, but then completely ignore the features.__\n",
    "\n",
    "`predict(X)` - Using the stored model parameters, predicts labels for features, X, and returns them. __But again, in this case, those predictions will not utilize the information of the features at all. The model will effectively be guessing.__\n",
    "\n",
    "`fit_predict(X, y)` - Sequentially performs the actions of `fit()` and `predict()`.\n",
    "\n",
    "Refer back to Part 1 if you need to. Here, we'll just instruct you on aspects that are specific to what we'll name the `FoolishClassifier` class. In addition to the `n_classes` argument that your `BayesClassifier`'s `__init__`method took, the `FoolishClassifier` will also need a `prior_distribution` argument. The `prior_distribution` argument should be one of three strings:\n",
    "\n",
    " - __`'uniform'`__: Your `fit` method literally does nothing. When your model makes predictions, it just makes a completely random guess, with equal probability for each class.\n",
    " - __`'mode'`__: Your `fit` method will determine the most frequent class of the labels, store it, and all its predictions will be of that class.\n",
    " - __`'proportional'`__: Your `fit` method's estimates of class prior probabilities are simply the fractional frequency of class occurances of the `labels` it is given, just as in Part 1 of the case study. For `predict` your model will randomly draw/sample from a distribution that has those priors (we'll guide you on how to do that)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FoolishClassifier.fit method\n",
    "\n",
    "You are welcome to implement all the methods at once. However, we've created test cells and visualizations and/or printouts that will guide you first through the testing of your `fit` method, then followed by ones for your `predict` method. So you may want to first work solely on `fit`, until you pass the test cell that follows. Then work on `predict` until is passes the subsequent test cell.\n",
    "\n",
    "Guidance on implementing `fit` is in the comments in the skeleton class below. Go for it!\n",
    "\n",
    "__You should use `numpy.random.choice()` to help with your `predict` method__. See guidance in a cell further below for more info, before coding your `predict`. Do not use other methods, e.g., `random.uniform()` or `numpy.random.uniform`, for making random predictions. Other methods would be fine, generally, but you'll need to use `choice` in order to pass the assert statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b38107b57420b488339d32af950d1a76",
     "grade": false,
     "grade_id": "ClassDefinition",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "## Implement the three methods of your Foolish classifier.\n",
    "\n",
    "class FoolishClassifier():\n",
    "    def __init__(self, prior_distribution, n_classes):\n",
    "        # Store the type of prior distribtion and number of\n",
    "        # classes as class attributes.\n",
    "        assert prior_distribution in ['uniform', 'mode', 'proportional']\n",
    "        self.prior_distribution = prior_distribution\n",
    "        self.n_classes = n_classes\n",
    "        # You may initialize other variables here if you want to,\n",
    "        # but it's not truly necessary. You can do that in .fit().\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "                \n",
    "        \n",
    "    def fit(self, features, labels):\n",
    "        # features - a numpy array of shape (n_samples, n_features)\n",
    "        # labels - a numpy array of shape (n_samples,)\n",
    "        #\n",
    "        # This method does not return/output anything.\n",
    "        \n",
    "        # Use 'labels' to fit your model parameters. Ignore 'features' altogether!\n",
    "        #\n",
    "        # If 'prior_distribution' is:\n",
    "        # 1. 'uniform' - Do nothing. There is no information in 'labels' that you need for predictions.\n",
    "        #                Just exit the function via a 'pass' or empty 'return' statement.\n",
    "        # 2. 'mode' - Determine which label/class occurs most frequently.\n",
    "        #             ** Save that label, the mode, as \"self.mode\".\n",
    "        # 3. 'proportional' - For each class, count the number of occurrances of\n",
    "        #                     that class and divide by the total number of samples\n",
    "        #                     to get the estimated prior for that class.\n",
    "        #                     ** Save the priors in a list or numpy array named \"self.priors\",\n",
    "        #                        ordering them by their class label (the prior for\n",
    "        #                        class 0 is first, the prior for n_classes-1 is last).\n",
    "        #\n",
    "        # As a reminder, if you use the range() function to loop over\n",
    "        # the class labels [0, 1, 2, ...] recall that range() excludes the\n",
    "        # value of the argument you give it. That is, range(4) will iterate\n",
    "        # over 0, 1, 2, and 3. It will exclude 4.\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "\n",
    "        \n",
    "    def predict(self, features):\n",
    "        # features - a numpy array of shape (n_samples, n_features)\n",
    "        #\n",
    "        # Returns an numpy array of predictions\n",
    "        \n",
    "        # Because the foolish classifier does not use features to make\n",
    "        # predictions, the only piece of information you need to extract\n",
    "        # from 'features' is the number of samples passed in, which is\n",
    "        # the number predictions that need to be made and returned.\n",
    "        #\n",
    "        # If 'prior_distribution' is:\n",
    "        # 1. 'uniform' - Draw predictions from a random distribution and\n",
    "        #                return them. You may use numpy.random.choice()\n",
    "        #                to help you.\n",
    "        # 2. 'mode' - Return an array of length equal to that of\n",
    "        #             the number of samples in 'features', with each\n",
    "        #             element being your stored mode value, 'self.mode'.\n",
    "        # 3. 'proportional' - Draw predictions from a probability disribution\n",
    "        #                     with probabilities equal to those of your\n",
    "        #                     stored 'self.priors'. You may use numpy.random.choice()\n",
    "        #                     to help you.\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        \n",
    "    def fit_predict(self, features, labels):\n",
    "        # features - a numpy array of shape (n_samples, n_features==2)\n",
    "        # labels - a numpy array of shape (n_samples,)\n",
    "        #\n",
    "        # Returns an array of predictions\n",
    "        \n",
    "        # Fit your model by calling self.fit(features, labels)\n",
    "        # Then make predictions by calling self.predict(features)\n",
    "        # Finally, return those predictions.\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below does a coarse check of your implementation of the FoolishClassifier's fit `method`. It may not give you much meaningful feedback to help you debug your code.\n",
    "\n",
    "To help with debugging, don't hesistate to:\n",
    "1. Put print statements in your code above, to show you the contents of variables and ensure they hold the values you expect. Also print out helpful info such as a variables type (e.g., are you sure it's a numpy array? Or is it a list?... `print(type(my_variable))`, shape: `print(my_variable.shape)`, and length: `print(len(my_variable))`. Note that the output of the print statements will appear below the cell in which your classifier is instantiated, or in which one of its methods is called. It will not appear under the cell in which the class is defined (above).\n",
    "2. Create a new cell below, and create simple data you can use to test your classifier (as in the autograder cell below).\n",
    "3. If you instantiate your class, you can then view its attributes and their values as shown below:\n",
    "```python\n",
    "# Create object\n",
    "clf = FoolishClassifier('proportional', n_classes=4)\n",
    "# print out the names of its attributes\n",
    "print(clf.__dict__.keys())\n",
    "# If an attribute is named \"self.priors\" you can print out its values...\n",
    "print(clf.priors)\n",
    "```\n",
    "Keep in mind that depending on how you implemented your class, some attributes may not exist until the `fit()` method is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "35a7f0a1fbe56bcba13c25c88f1979c2",
     "grade": true,
     "grade_id": "Test_fit",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder: Test FoolishClassifier.fit()\n",
    "np.random.seed(42)\n",
    "n_classes = 4\n",
    "y = np.random.choice(n_classes, size=1000)\n",
    "X = None\n",
    "\n",
    "# Test 'uniform'\n",
    "clf = FoolishClassifier('uniform', n_classes)\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Test 'mode'\n",
    "clf = FoolishClassifier('mode', n_classes)\n",
    "clf.fit(X, y)\n",
    "print(clf.mode)\n",
    "assert clf.mode==3\n",
    "\n",
    "# Test 'proportional'\n",
    "clf = FoolishClassifier('proportional', n_classes)\n",
    "clf.fit(X, y)\n",
    "print(clf.priors)\n",
    "assert np.all(np.isclose(clf.priors, [0.258, 0.23, 0.232, 0.28]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FoolishClassifier.predict and fit_predict methods\n",
    "\n",
    "If you didn't implement `predict` and `fit_predict` already, now is the time to do so. The methods will be tested in the test cell below. Guidance is provided in the comments in the class skeleton.\n",
    "\n",
    "__You should use `numpy.random.choice()` to help with your `predict` method__. The function draws sample from a discrete probability function that you define. Read the [numpy.random.choice documentation](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) carefully. Because we want each sample to be independently chosen/sampled/drawn, what is the correct setting for the `replace` argument?\n",
    "\n",
    "For those with greater math and programming skills, you may want to challenge yourself... Think about how you would implement `predict` if `numpy.random.choice` was not available, and you only had a uniform random number generator (random floating-point numbers between 0 and 1) available to you. (Don't actually use such an implementation, however, since you will almost certainly fail the assert statements if you do, since they rely on the known results from `numpy.random`, with the seed it is given.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5ef5d8f0f0cf650278877dc272a2cd74",
     "grade": true,
     "grade_id": "Test_predict",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder: Test FoolishClassifier.predict() and fit_predict()\n",
    "np.random.seed(42)\n",
    "n_classes = 4\n",
    "y = np.array([0]*100 + [1]*200 + [2]*300 + [3]*400)\n",
    "np.random.shuffle(y)\n",
    "X = np.zeros((1000, 10))\n",
    "\n",
    "# Test 'uniform'\n",
    "clf = FoolishClassifier('uniform', n_classes)\n",
    "clf.fit(X, y)\n",
    "predictions = clf.predict(X)\n",
    "label, counts = np.unique(predictions, return_counts=True)\n",
    "print(counts)\n",
    "assert np.array_equal(counts, [267, 248, 257, 228])\n",
    "\n",
    "# Test 'mode'\n",
    "clf = FoolishClassifier('mode', n_classes)\n",
    "clf.fit(X, y)\n",
    "predictions = clf.predict(X)\n",
    "label, counts = np.unique(predictions, return_counts=True)\n",
    "assert label[0]==clf.mode\n",
    "print(counts)\n",
    "assert np.array_equal(counts, [1000])\n",
    "\n",
    "# Test 'proportional'\n",
    "clf = FoolishClassifier('proportional', n_classes)\n",
    "clf.fit(X, y)\n",
    "predictions = clf.predict(X)\n",
    "label, counts = np.unique(predictions, return_counts=True)\n",
    "print(counts)\n",
    "assert np.array_equal(counts, [103, 194, 287, 416])\n",
    "\n",
    "# Test fit_predict\n",
    "clf = FoolishClassifier('mode', n_classes)\n",
    "predictions = clf.fit_predict(X, y)\n",
    "label, counts = np.unique(predictions, return_counts=True)\n",
    "assert label[0]==clf.mode\n",
    "print(counts)\n",
    "assert np.array_equal(counts, [1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and visualization\n",
    "\n",
    "We'll create a synthetic data set, as we did last time, then you'll train four models on the train split, and make predictions on the test set split. Then we'll plot those predictions. The four models you'll train and test will be:\n",
    "1. Gaussian Naive Bayes\n",
    "2. FoolishClassifier with uniform distribution\n",
    "3. FoolishClassifier with mode distribution\n",
    "4. FoolishClassifier with proportional distribution\n",
    "\n",
    "The Gaussian Naive Bayes class, [GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) was imported for you at that beginning of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create random samples of three classes, with samples drawn from multivariate Gaussian distributions\n",
    "np.random.seed(100)\n",
    "n1 = 80\n",
    "mean1 = [1.5, 3]\n",
    "cov1 = [[0.5, 0], [0, 0.5]]\n",
    "n2 = 10\n",
    "mean2 = [3, 2]\n",
    "cov2 = [[0.1, 0], [0, 1]]\n",
    "n3 = 10\n",
    "mean3 = [0, 1]\n",
    "cov3 = [[1, 0], [0, 1]]\n",
    "features = [np.random.multivariate_normal(mean1, cov1, size=n1),\n",
    "            np.random.multivariate_normal(mean2, cov2, size=n2),\n",
    "            np.random.multivariate_normal(mean3, cov3, size=n3),\n",
    "           ]\n",
    "features = np.concatenate(features, axis=0)\n",
    "labels = np.array([0]*n1 + [1]*n2 + [2]*n3)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, random_state=100, test_size=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "96b238e3a53a146bc7d5e64fa24a13af",
     "grade": false,
     "grade_id": "Make_predictions",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# In this cell, instantiate the four models. Your FoolishClassifier\n",
    "# models should be instantiated with n_classes=3.\n",
    "#\n",
    "# Train your models using training features, X_train, and labels, y_train.\n",
    "# Then, predict the test set labels using X_test. Save your predictions\n",
    "# to variables with the following names:\n",
    "#    pred_nb, pred_uniform, pred_mode, pred_proportional\n",
    "\n",
    "n_classes = 3\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll plot your results for you in the cell below. Do they look plausible to you, given what each model is doing? In the subsequent cell we'll test your predictions with some `assert` statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 12))\n",
    "plt.clf()\n",
    "\n",
    "plt.subplot(3, 2, 1)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=y_test, s=50, edgecolor='k')\n",
    "plt.title('True test data categories')\n",
    "\n",
    "plt.subplot(3, 2, 3)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=pred_nb, s=50, edgecolor='k')\n",
    "plt.title('Naive Bayes predictions')\n",
    "\n",
    "plt.subplot(3, 2, 4)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=pred_uniform, s=50, edgecolor='k')\n",
    "plt.title('Foolish-uniform predictions')\n",
    "\n",
    "plt.subplot(3, 2, 5)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=pred_mode, s=50, edgecolor='k')\n",
    "plt.title('Foolish-mode predictions')\n",
    "\n",
    "plt.subplot(3, 2, 6)\n",
    "plt.scatter(X_test[:, 0], X_test[:, 1], marker='o', c=pred_proportional, s=50, edgecolor='k')\n",
    "_ = plt.title('Foolish-proportional predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "957619ba5d3b9e4300114e8b8f64056b",
     "grade": true,
     "grade_id": "Test_test_set_predictions",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder: Test predictions of all classifiers, assuring\n",
    "# they match the expected results (which may or may not be\n",
    "# the true classes/labels).\n",
    "\n",
    "n_correct_nb = np.sum(y_test==pred_nb)\n",
    "print(n_correct_nb)\n",
    "assert n_correct_nb==43\n",
    "\n",
    "n_correct_uniform = np.sum(y_test==pred_uniform)\n",
    "print(n_correct_uniform)\n",
    "assert n_correct_uniform==19 or n_correct_uniform==14\n",
    "\n",
    "n_correct_mode = np.sum(y_test==pred_mode)\n",
    "print(n_correct_mode)\n",
    "assert n_correct_mode==40\n",
    "\n",
    "n_correct_proportional = np.sum(y_test==pred_proportional)\n",
    "print(n_correct_proportional)\n",
    "assert n_correct_proportional==35 or n_correct_proportional==33"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "As in Part 1, we'll use the following to help us evaluate model performance:\n",
    "- Accuracy scores\n",
    "- Confusion matrices\n",
    "- Precision, Recall, and F-scores\n",
    "\n",
    "You have three more coding tasks... (1) implementing your own function for computing accuracy, (2) implementing your own function for computing a confusion matrix, and (3) using scikit-learn to compute F-scores. You may not use scikit-learn for 1 and 2 (though you may want to use the scikit-learn version during development, to make sure your results match those of scikit-learn).\n",
    "\n",
    "We'll start off with the accuracy function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4889909bc35cb5a95f1fc8130af0e48f",
     "grade": false,
     "grade_id": "Accuracy_function",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Implement your accuracy function.\n",
    "# You may not use scikit-learn in your implementation.\n",
    "def my_accuracy_score(y_true, y_pred):\n",
    "    # y_true: An iterable of true label values.\n",
    "    # y_pred: An iterable of predicted label values.\n",
    "    #\n",
    "    # Returns the fraction of predicted label values that match the true label values,\n",
    "    # a number between 0 and 1.\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how accurate the models are, and test your code to make sure the accuracy scores are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_nb = my_accuracy_score(y_test, pred_nb)\n",
    "acc_uniform = my_accuracy_score(y_test, pred_uniform)\n",
    "acc_mode = my_accuracy_score(y_test, pred_mode)\n",
    "acc_proportional = my_accuracy_score(y_test, pred_proportional)\n",
    "\n",
    "print(f'Naive Bayes accuracy:          {acc_nb:0.3f}')\n",
    "print(f'Foolish-uniform accuracy:      {acc_uniform:0.3f}')\n",
    "print(f'Foolish-mode accuracy:         {acc_mode:0.3f}')\n",
    "print(f'Foolish-proportional accuracy: {acc_proportional:0.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "591d96de0338133a6656674fe57ad2f5",
     "grade": true,
     "grade_id": "Test_accuracy_function",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder: Confirm that accuracy scores were computed correctly\n",
    "assert np.isclose(acc_nb, 0.86)\n",
    "assert np.isclose(acc_uniform, 0.38) or np.isclose(acc_uniform, 0.28)\n",
    "assert np.isclose(acc_mode, 0.80)\n",
    "assert np.isclose(acc_proportional, 0.70) or np.isclose(acc_proportional, 0.66)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the accuracy scores, and at the scatter plots of predictions (several cells above). Do they make sense?\n",
    "\n",
    "The Foolish classifier is very foolish. It doesn't use the features at all when making its predictions. __Yet, the accuracy scores of the 'mode' classifier and the 'proportional' classifier are almost as high as that of the Naive Bayes model!__ Ask yourself why that is, and if it isn't clear be sure to ask the instructional team about it.\n",
    "\n",
    "At this point it is hopefully evident that accuracy is sometimes a poor metric for evaluating a model's performance.\n",
    "\n",
    "Let's move on to your confusion matrix function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1110046a351bb144a32a8f3a5a4f8e17",
     "grade": false,
     "grade_id": "ConfusionMat_function",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Implement your confusion matrix function.\n",
    "# You may not use scikit-learn in your implementation.\n",
    "def my_confusion_matrix(y_true, y_pred, n_classes):\n",
    "    # y_true: An iterable of true label values.\n",
    "    # y_pred: An iterable of predicted label values.\n",
    "    # n_classes: The number of classes. User-provided labels in y_true and\n",
    "    #            y_pred must be values from 0 to n_classes-1.\n",
    "    #\n",
    "    # Returns a confusion matrix numpy array of shape (n_classes, n_classes).\n",
    "    #   For row i, column j, the confusion matrix contains the number of\n",
    "    #   samples that had true label i, and predicted label j.\n",
    "    \n",
    "    # Guidance:\n",
    "    # There are many ways to do this, but here is one approach:\n",
    "    # 1. Initialize a confusion matrix with all zeros, using numpy.zeros().\n",
    "    # 2. Use a for loop to loop over the rows (true classes)\n",
    "    # 3. Within that loop, use a for loop to loop over the columns (predicted classes)\n",
    "    # 4. Using the row and column loop counters, examine each sample in\n",
    "    #    y_true and y_pred and see if they are of the class for that row\n",
    "    #    and column. If so, add 1 to the confusion matrix at that row and column.\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the confusion matrices produced by your function, and test your code to make sure they are correct.\n",
    "\n",
    "__Recall that a good model will produce a confusion matrix that has larger values along the diagonal and smaller (ideally 0) values off the diagonal.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_nb = my_confusion_matrix(y_test, pred_nb, n_classes)\n",
    "cm_uniform = my_confusion_matrix(y_test, pred_uniform, n_classes)\n",
    "cm_mode = my_confusion_matrix(y_test, pred_mode, n_classes)\n",
    "cm_proportional = my_confusion_matrix(y_test, pred_proportional, n_classes)\n",
    "\n",
    "print('Naive Bayes')\n",
    "print(cm_nb)\n",
    "\n",
    "print('\\nFoolish-uniform')\n",
    "print(cm_uniform)\n",
    "\n",
    "print('\\nFoolish-mode')\n",
    "print(cm_mode)\n",
    "\n",
    "print('\\nFoolish-proportional')\n",
    "print(cm_proportional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "380cdf74d785145c04ba2b3f373edc81",
     "grade": true,
     "grade_id": "Test_ConfusionMat_function",
     "locked": true,
     "points": 15,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder: Confirm that confusion scores were computed correctly\n",
    "assert int(cm_nb[0, 0])==38\n",
    "assert int(cm_uniform[2, 2])==2\n",
    "assert int(cm_mode[1, 0])==5\n",
    "assert int(cm_proportional[0, 1])==6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the confusion matrices. __Can you see how they are more informative than accuracy scores, and indicate what types of errors the classifiers are making?__\n",
    "\n",
    "The Foolish-mode confusion matrix has all zeros in columns 1 and 2, making it clear that the classifier __never__ predicts a sample to be of class 1 and 2. Bad news! And while the Foolish-proportional classifier sometimes predicts a sample to be of class 1 and 2, those predictions are usually incorrect. Finally, we see that the Foolish-uniform classifier is often predicting true samples of class 0 to be of class 1 or 2 (the off-diagonal counts in the top row).\n",
    "\n",
    "Now that you've observed how informative a confusion matrix is, we'll have you look at precision, recall, and F-score. You can use the scikit-learn [precision_recall_fscore_support](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html) function rather than building your own."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision, Recall, and F-score\n",
    "\n",
    "You may want to review what precision, recall, and F-score are by reading the header of the scikit-learn documentation (see link above). In short...\n",
    "\n",
    "- __Precision__ measures how often a sample predicted to be of class i is truly of class i.\n",
    "- __Recall__ measures how often a sample that is __not__ of class i is correctly predicted to be __not__ of class i.\n",
    "- __F-score__ is the harmonic mean of precision and recall: (2 x precision x recall) / (precision + recall). This means that if either precision or recall are low, then f-score will be low. __F-score is a very effective metric for model performance on multiclass classification tasks.__\n",
    "\n",
    "Note that precision, recall, and F-score are computed for each class, so you'll get three values for each, for each classifier. __To prevent the analysis for getting to messy, we'll ask that you only save the three f-scores for each classifier, and examine them.__ You may of course save and print out the precision and recall scores as well, for your own independent observation.\n",
    "\n",
    "You will get an `UndefinedMetricWarning` which indicates that a particular class/label was never predicted (e.g., by the Foolish-mode classifier) and thus division by zero would occur in the computations. You should read the warning, but its presence is expected and of no concern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "63bf2283eaa81133619f8816a1a89a45",
     "grade": false,
     "grade_id": "Compute_Fscores",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Use scikit-learn to compute the fscore for each of your classifiers.\n",
    "# Save results to variables named: fscore_nb, fscore_uniform,\n",
    "# fscore_mode, and fscore_proportional.\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's observe your F-scores, and make sure they are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Naive Bayes')\n",
    "print(fscore_nb)\n",
    "\n",
    "print('\\nFoolish-uniform')\n",
    "print(fscore_uniform)\n",
    "\n",
    "print('\\nFoolish-mode')\n",
    "print(fscore_mode)\n",
    "\n",
    "print('\\nFoolish-proportional')\n",
    "print(fscore_proportional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e929df926a7357549409bd62cbc2183b",
     "grade": true,
     "grade_id": "Test_Fscores",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Autograder: Test the F-scores for correctness\n",
    "assert np.isclose(fscore_nb[0], 0.92682927)\n",
    "assert np.isclose(fscore_uniform[1], 0.47058824) or np.isclose(fscore_uniform[1], 0.09090909)\n",
    "assert np.isclose(fscore_mode[0], 0.88888889)\n",
    "assert np.isclose(fscore_proportional[2], 0.33333333) or np.isclose(fscore_proportional[2], 0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note that based on F-score, all three Foolish classifiers performed much worse than the Naive Bayes classifier__, when one takes into consideration the individual scores for each class. If one wanted to, taking the average F-score across all three classes would provide a meaningfull __scalar-valued__ metric of overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're done with Case Study 2!\n",
    "\n",
    "__Hopefully you now have a much better understanding of ML models, including implementation, fitting/training, predicting, and ways of evaluating performance (without being fooled by high scores on the more simplistic evaluation metrics).__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "32a8c7a18d041ffd1b7e27ee6c95c484",
     "grade": false,
     "grade_id": "Feedback",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def feedback():\n",
    "    \"\"\"Provide feedback on the contents of this exercise\n",
    "    \n",
    "    Returns:\n",
    "        string\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c34c64dab434b0766fa5e68ab5725830",
     "grade": true,
     "grade_id": "Feedback_answer",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
