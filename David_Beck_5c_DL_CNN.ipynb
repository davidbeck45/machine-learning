{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UMmWEj1JnxEF"
      },
      "source": [
        "## CSCI 470 Activities and Case Studies\n",
        "\n",
        "1. For all activities, you are allowed to collaborate with a partner. \n",
        "1. For case studies, you should work individually and are **not** allowed to collaborate.\n",
        "\n",
        "By filling out this notebook and submitting it, you acknowledge that you are aware of the above policies and are agreeing to comply with them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZBouvo9nxEH"
      },
      "source": [
        "Some considerations with regard to how these notebooks will be graded:\n",
        "\n",
        "1. Cells in which \"# YOUR CODE HERE\" is found are the cells where your graded code should be written.\n",
        "2. In order to test out or debug your code you may also create notebook cells or edit existing notebook cells other than \"# YOUR CODE HERE\". We actually highly recommend you do so to gain a better understanding of what is happening. However, during grading, **these changes are ignored**. \n",
        "2. You must ensure that all your code for the particular task is available in the cells that say \"# YOUR CODE HERE\"\n",
        "3. Every cell that says \"# YOUR CODE HERE\" is followed by a \"raise NotImplementedError\". You need to remove that line. During grading, if an error occurs then you will not receive points for your work in that section.\n",
        "4. If your code passes the \"assert\" statements, then no output will result. If your code fails the \"assert\" statements, you will get an \"AssertionError\". Getting an assertion error means you will not receive points for that particular task.\n",
        "5. If you edit the \"assert\" statements to make your code pass, they will still fail when they are graded since the \"assert\" statements will revert to the original. Make sure you don't edit the assert statements.\n",
        "6. We may sometimes have \"hidden\" tests for grading. This means that passing the visible \"assert\" statements is not sufficient. The \"assert\" statements are there as a guide but you need to make sure you understand what you're required to do and ensure that you are doing it correctly. Passing the visible tests is necessary but not sufficient to get the grade for that cell.\n",
        "7. When you are asked to define a function, make sure you **don't** use any variables outside of the parameters passed to the function. You can think of the parameters being passed to the function as a hint. Make sure you're using all of those variables.\n",
        "8. Finally, **make sure you run \"Kernel > Restart and Run All\"** and pass all the asserts before submitting. If you don't restart the kernel, there may be some code that you ran and deleted that is still being used and that was why your asserts were passing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "47ccadb4be48caa31405858fec2162fb",
          "grade": false,
          "grade_id": "cell-8624ebbfca3f9d05",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "59BOxjcnnxEI"
      },
      "source": [
        "# Deep Learning - Convolutional Neural Networks\n",
        "\n",
        "In this exercise we'll compare a simple fully-connected (dense) feedforward neural network with a convolutional neural network at predicting the [MNIST handwritten digits, drawn in a 28x28 pixel image](http://yann.lecun.com/exdb/mnist/). \n",
        "\n",
        "Note that each sample we're using is a grayscale image - that means that each sample (image) is a matrix of values whereas previously our samples had been vectors. You will need to modify the data to work with this accordingly. For example, keras' [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers expect their inputs to be vectors so whenever you're using a [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layer, make sure you convert its inputs to a vector, if needed. In keras, you can convert a matrix (or tensor) to a vector using the [Flatten](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) layer. For more complex changes where you want to cusomize the exact shape of the values, you can use the [Reshape](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape) layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-69b5ab127f73f4ab",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "id": "29N6sVognxEI"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10, mnist\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "import numpy as np\n",
        "import scipy\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-2f12505f86743d95",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVMK2WENnxEJ",
        "outputId": "1e2b95e6-e673-4934-9aa1-57955fb57637"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-43ee2cdae7268658",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "id": "DrBHNse_nxEJ"
      },
      "outputs": [],
      "source": [
        "s1 = x_train.shape\n",
        "s2 = x_test.shape\n",
        "print(f\"The MNIST data was loaded with {s1[0]} training samples and {s2[0]} testing samples.\")\n",
        "print(f\"Each sample is a {s1[1]} x {s1[2]} pixel image.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-5c9c291b795e1704",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pFLkyqiPnxEK",
        "outputId": "f79973a1-97a1-42b3-c42b-064c2e7113a1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
              "         18,  18,  18, 126, 136, 175,  26, 166, 255, 247, 127,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  30,  36,  94, 154, 170,\n",
              "        253, 253, 253, 253, 253, 225, 172, 253, 242, 195,  64,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  49, 238, 253, 253, 253, 253,\n",
              "        253, 253, 253, 253, 251,  93,  82,  82,  56,  39,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,  18, 219, 253, 253, 253, 253,\n",
              "        253, 198, 182, 247, 241,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  80, 156, 107, 253, 253,\n",
              "        205,  11,   0,  43, 154,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,  14,   1, 154, 253,\n",
              "         90,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 139, 253,\n",
              "        190,   2,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  11, 190,\n",
              "        253,  70,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  35,\n",
              "        241, 225, 160, 108,   1,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "         81, 240, 253, 253, 119,  25,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  45, 186, 253, 253, 150,  27,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,  16,  93, 252, 253, 187,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0, 249, 253, 249,  64,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,  46, 130, 183, 253, 253, 207,   2,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  39,\n",
              "        148, 229, 253, 253, 253, 250, 182,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  24, 114, 221,\n",
              "        253, 253, 253, 253, 201,  78,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,  23,  66, 213, 253, 253,\n",
              "        253, 253, 198,  81,   2,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,  18, 171, 219, 253, 253, 253, 253,\n",
              "        195,  80,   9,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,  55, 172, 226, 253, 253, 253, 253, 244, 133,\n",
              "         11,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0, 136, 253, 253, 253, 212, 135, 132,  16,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0],\n",
              "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0]], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Here's an example hand drawn digit's image\n",
        "example = x_train[0]\n",
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ll0XPBGHnxEK"
      },
      "outputs": [],
      "source": [
        "# Now let's plot that matrix to better understand what's happening here...\n",
        "_ = plt.imshow(example, cmap=\"gray\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bz0mriuunxEK"
      },
      "outputs": [],
      "source": [
        "# In this cell, we'll print out the unique labels in the data.\n",
        "#\n",
        "# We're predicting the digit in an image and we have images of\n",
        "# all 10 ('0' to '9') digits.\n",
        "\n",
        "print(np.unique(y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JF_87iLknxEK"
      },
      "source": [
        "### Convolutions\n",
        "\n",
        "Most engineers in computer scientists are familiar with convolutions. If you're not, or you need a refresher, try [this article on discrete convolutions](https://electricalacademia.com/signals-and-systems/example-of-discrete-time-graphical-convolution/). The article is on discrete *time* convolutions, but the process is the same for discrete *spatial* convolutions--which is what we are doing when we perform a 2D convolution on an image.\n",
        "\n",
        "To check your basic understanding of convolutions, implement the function below, which computes what the output image shape will be for a given convolutional kernel (filter) applied to a given input image. Recall that for a given dimension, the length of the output of a convolution is:\n",
        "\n",
        "`L_out = L_in - L_kernel + 1`\n",
        "\n",
        "where L_in is the length of the input (pixels), L_kernel is the length of the filter/kernel, and L_out is the length of the output (pixels)\n",
        "\n",
        "The equation above assumes that we haven't added any \"padding\" to the input image, e.g., pixels of value 0 that we use to extend/enlarge the input image. In Keras, you'll see that a user can specify padding as a scalar number of pixels to add __to each side of the image__. Thus,\n",
        "\n",
        "`L_in_padded = L_in + 2*padding`  \n",
        "`L_out = L_in_padded - L_kernel + 1`  \n",
        "`L_out = L_in + 2*padding - L_kernel + 1`  \n",
        "\n",
        "Finally, we may want to use a convolution in which the kernel doesn't \"slide\" along the input in single-pixel steps, but in multiple-pixel steps, of step size `stride`. You may want to diagram it out for yourself on a piece of paper (in a manner like that of the article linked above), but we effectively just need to divide the output length by the stride value (before adding the 1) to get strided output size. Because it may be fractional, the final answer is actually the floor of that previous result. This gives us...\n",
        "\n",
        "`L_out = floor( (L_in + 2*padding - L_kernel) / stride + 1 )`\n",
        "\n",
        "In the function below, you'll need to implement that equation separately for the horizontal and vertical dimensions of the image and kernel. Do not assume that either the image or the kernel are square. In Python, `int()` converts a number to an integer, executing a floor operation if the number is floating-point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "181818b1f77d0f52ace8cfea1f63a385",
          "grade": false,
          "grade_id": "cell-887e94c3f19aac00",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "0i_-GXjGnxEK"
      },
      "outputs": [],
      "source": [
        "def calculate_conv_shape(X, K, padding=0, stride=1):\n",
        "    \"\"\"Calculate the shape of the output of a convolution \n",
        "    \n",
        "    Args:\n",
        "        X (np.array): The input matrix\n",
        "        K (np.array): The filter matrix\n",
        "        padding (int, optional): Defaults to 0. The padding dimension\n",
        "        stride (int, optional): Defaults to 1. The stride of the convolution\n",
        "    \n",
        "    Returns:\n",
        "        tuple: The shape of the convolution output, height then width\n",
        "    \"\"\"\n",
        "    input_height, input_width = X.shape\n",
        "    kernel_height, kernel_width = K.shape\n",
        "    \n",
        "    output_height = int(((input_height + 2*padding - kernel_height) / stride) + 1)\n",
        "    output_width = int(((input_width + 2*padding - kernel_width) / stride) + 1)\n",
        "    \n",
        "    return output_height, output_width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-f77d68a4c1196592",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_8_ejpdjnxEL",
        "outputId": "b30de130-3311-4115-a8f1-3f88075ced3c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "# Here, we define a \"blurring\" filter/kernel, that can be applied\n",
        "# to an image to get a blurred output image.\n",
        "\n",
        "blur = np.array([\n",
        "    [0,    0.25, 0   ],\n",
        "    [0.25, 0.5,  0.25],\n",
        "    [0,    0.25, 0   ]\n",
        "])\n",
        "\n",
        "# If we pad our 28x28 example image and then convolve it\n",
        "# with the blurring kernel (with stride=1), the output\n",
        "# image should also be 28x28.\n",
        "calculate_conv_shape(example, blur, padding=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4a69c868df7984fbcf05d18782858c39",
          "grade": true,
          "grade_id": "cell-8c49e7193a849321",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "XVFWAu-LnxEL"
      },
      "outputs": [],
      "source": [
        "ans = calculate_conv_shape(example, blur, padding=1)\n",
        "assert isinstance(ans, tuple)\n",
        "assert isinstance(ans[0], int)\n",
        "assert isinstance(ans[1], int)\n",
        "assert ans == (28, 28)\n",
        "ans = calculate_conv_shape(example, blur, padding=0, stride=2)\n",
        "assert ans == (13, 13)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "926290a0f7c73ec67c02a53e2deb0fe3",
          "grade": false,
          "grade_id": "cell-25dbc77554a4c538",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "6KMeikSEnxEL"
      },
      "source": [
        "### Try out a convolution\n",
        "\n",
        "To apply a convolution, you can use the [scipy.ndimage.convolve](https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.convolve.html). `scipy` has already been imported for you above.\n",
        "\n",
        "Convert `example` to floating point numbers using `example.astype(np.float)` before you execute the convolution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "d28950dc98905e27a1ad46219750ebec",
          "grade": false,
          "grade_id": "cell-eaf956c226dd39b7",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "f35sFx7onxEL",
        "outputId": "d3d13c8a-2c34-47df-b04f-0d39d00fb96d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Apply the blurring filter to the example and save the output to \"blurred_image\".\n",
        "#\n",
        "# Be sure to convert the example from unsigned int numbers to floating-point numbers\n",
        "# beforehand.\n",
        "from scipy.signal import convolve2d\n",
        "\n",
        "# Convert example to floating-point numbers\n",
        "example = example.astype(float)\n",
        "\n",
        "# Define the blurring filter\n",
        "blurring_filter = np.ones((3, 3)) / 9\n",
        "\n",
        "# Apply the blurring filter\n",
        "blurred_image = convolve2d(example, blurring_filter, mode='same')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(example, cmap=\"gray\")\n",
        "plt.colorbar()\n",
        "           \n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(blurred_image, cmap=\"gray\")\n",
        "_ = plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cbe761b38b1262f56ec7e75a94c7a54a",
          "grade": false,
          "grade_id": "cell-1051e90fd643f6ab",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "bikNW6xqnxEL",
        "outputId": "4dc651f7-29f3-4063-e428-cc0bd2cc7b35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-bbeeb621ccee>:14: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  example = example.astype(np.float)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 4 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0EAAAGdCAYAAADUnnikAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABONklEQVR4nO3dfXhU9Z3//1cCJAHMTBwwmYwGjDfl/qYbMEaRxZISAuUrkm6L8rPRcsHWTdhitkjphdx5k4pWKTTK2hvQXVDrXgWVddEYJNES7sJSBG0KiBKFSRRMBkJJQjK/P7wy6wgcTjiTzEnm+biuc13MOe/MfE6ieeV9Pp85E+X3+/0CAAAAgAgRHe4BAAAAAEBHogkCAAAAEFFoggAAAABEFJogAAAAABGFJggAAABARKEJAgAAABBRaIIAAAAARBSaIAAAAAARpXu4BwAAndnZs2fV2NgYkueKiYlRXFxcSJ4LABC5yKZLowkCgMt09uxZpaamyuv1huT53G63jhw50iXDBgDQMcgmc2iCAOAyNTY2yuv16ujRo3I4HJaey+fzqV+/fmpsbOxyQQMA6Dhkkzk0QQBgkcPhsBw0AACEEtlkjBsjAIBFfr8/JFtbFBYWavTo0YqPj1diYqKmTp2qysrKoJpx48YpKioqaPvJT34SVHP06FFNnjxZvXr1UmJioubNm6dz585Z/p4AAMIrHNnUmTATBAAWhSIo2vr1paWlysvL0+jRo3Xu3Dn94he/0IQJE/TBBx+od+/egbpZs2Zp2bJlgce9evUK/Lu5uVmTJ0+W2+3Wtm3bdPz4cf3oRz9Sjx499Nhjj1k6HwBAeIUjmzoTmiAA6IQ2b94c9Hjt2rVKTExURUWFxo4dG9jfq1cvud3uCz7HW2+9pQ8++EBvv/22kpKSNHLkSD388MOaP3++lixZopiYmHY9BwAAwoXlcABgUSiXHPh8vqCtoaHB1Bjq6uokSS6XK2j/unXr1LdvXw0dOlQLFizQmTNnAsfKy8s1bNgwJSUlBfZlZWXJ5/PpwIEDVr8tAIAwYjmcMWaCAMCiUC45SElJCdq/ePFiLVmyxPBrW1paNHfuXN16660aOnRoYP/dd9+t/v37y+PxaN++fZo/f74qKyv1pz/9SZLk9XqDGiBJgcehurUqACA8WA5njCYIAGykqqoq6G4+sbGxl/yavLw87d+/X++9917Q/tmzZwf+PWzYMCUnJ2v8+PE6fPiwrr/++tANGgCAToYmCAAsCuXVtrbe0jQ/P1+bNm1SWVmZrrnmGsPa9PR0SdKhQ4d0/fXXy+12a+fOnUE11dXVknTR9xEBADoHZoKM8Z4gALAoHOuu/X6/8vPztWHDBm3ZskWpqamX/Jq9e/dKkpKTkyVJGRkZev/991VTUxOoKS4ulsPh0ODBg9s0HgCAvfCeIGPMBAFAJ5SXl6f169fr1VdfVXx8fOA9PE6nUz179tThw4e1fv16TZo0SX369NG+ffv0wAMPaOzYsRo+fLgkacKECRo8eLDuueceLV++XF6vVwsXLlReXp6pZXgAAHRWNEEAYFE4lhw8++yzkr76QNSvW7Nmje69917FxMTo7bff1ooVK1RfX6+UlBTl5ORo4cKFgdpu3bpp06ZNuv/++5WRkaHevXsrNzc36HOFAACdE8vhjNEEAYBF4QiaS9WnpKSotLT0ks/Tv39/vfHGG216bQCA/dEEGeM9QQAAAAAiCjNBAGARV9sAAHZDNhmjCQIAiwgaAIDdkE3GWA4HAAAAIKIwEwQAFnG1DQBgN2STMZogALCIoAEA2A3ZZIzlcAAAAAAiCjNBAGARV9sAAHZDNhmjCQIAiwgaAIDdkE3GWA4HAAAAIKIwEwQAFnG1DQBgN2STMWaCAMCi1qCxugEAECrhyKaysjJNmTJFHo9HUVFR2rhx40Vrf/KTnygqKkorVqwI2n/y5EnNmDFDDodDCQkJmjlzpk6fPh1Us2/fPt12222Ki4tTSkqKli9f3qZxSjRBAAAAAEKgvr5eI0aMUFFRkWHdhg0btH37dnk8nvOOzZgxQwcOHFBxcbE2bdqksrIyzZ49O3Dc5/NpwoQJ6t+/vyoqKvTEE09oyZIleu6559o0VpbDAYBFLDkAANhNOLIpOztb2dnZhjWfffaZ5syZozfffFOTJ08OOvbhhx9q8+bN2rVrl0aNGiVJWrVqlSZNmqQnn3xSHo9H69atU2Njo/7whz8oJiZGQ4YM0d69e/XUU08FNUuXwkwQAIQAS+EAAHYTqmzy+XxBW0NDw2WNp6WlRffcc4/mzZunIUOGnHe8vLxcCQkJgQZIkjIzMxUdHa0dO3YEasaOHauYmJhATVZWliorK/Xll1+aHgtNEAAAAICLSklJkdPpDGyFhYWX9TyPP/64unfvrn/913+94HGv16vExMSgfd27d5fL5ZLX6w3UJCUlBdW0Pm6tMYPlcABgEcvhAAB2E8psqqqqksPhCOyPjY1t83NVVFTo17/+tfbs2aOoqChL4woFZoIAwCLuDgcAsJtQZpPD4QjaLqcJevfdd1VTU6N+/fqpe/fu6t69uz755BP927/9m6699lpJktvtVk1NTdDXnTt3TidPnpTb7Q7UVFdXB9W0Pm6tMYMmCAAAAEC7uueee7Rv3z7t3bs3sHk8Hs2bN09vvvmmJCkjI0O1tbWqqKgIfN2WLVvU0tKi9PT0QE1ZWZmampoCNcXFxRowYICuvPJK0+NhORwAWMRyOACA3YQjm06fPq1Dhw4FHh85ckR79+6Vy+VSv3791KdPn6D6Hj16yO12a8CAAZKkQYMGaeLEiZo1a5ZWr16tpqYm5efna/r06YHbad99991aunSpZs6cqfnz52v//v369a9/raeffrpNY6UJAgCLaIIAAHYTjmzavXu3br/99sDjgoICSVJubq7Wrl1r6jnWrVun/Px8jR8/XtHR0crJydHKlSsDx51Op9566y3l5eUpLS1Nffv21aJFi9p0e2yJJggAAABACIwbN65NjdPHH3983j6Xy6X169cbft3w4cP17rvvtnV4QWiCAMAiZoIAAHZDNhmjCQIAiwgaAIDdkE3GuDscAAAAgIjCTBAAWMTVNgCA3ZBNxmiCAMAiggYAYDdkkzGWwwEAAACIKMwEAYBFXG0DANgN2WSMJggALCJoAAB2QzYZYzkcAAAAgIjCTBAAWMTVNgCA3ZBNxmiCAMAiggYAYDdkkzGWwwEAAACIKMwEAYBFXG0DANgN2WSMJggALCJoAAB2QzYZs10T1NLSomPHjik+Pl5RUVHhHg6ALsjv9+vUqVPyeDyKjmZVMC6NbALQ3simjmW7JujYsWNKSUkJ9zAARICqqipdc801lp+Hq21dH9kEoKOQTR2j3drMoqIiXXvttYqLi1N6erp27txp6uvi4+Pba0gAECRUv29ag8bqhvZ1ubkkkU0AOg7Z1DHapQl6+eWXVVBQoMWLF2vPnj0aMWKEsrKyVFNTc8mvZZkBgI7C75vIYSWXJP5bAdBx+H3TMdqlCXrqqac0a9Ys3XfffRo8eLBWr16tXr166Q9/+EN7vBwAhBVX2+yPXAIQacgmYyFvghobG1VRUaHMzMz/e5HoaGVmZqq8vPy8+oaGBvl8vqANADobQsa+2ppLEtkEoGsgmy4u5E3QF198oebmZiUlJQXtT0pKktfrPa++sLBQTqczsPHGUwBAKLU1lySyCQC6urDff2/BggWqq6sLbFVVVeEeEgC0CUsOuh6yCUBnRzYZC/ktsvv27atu3bqpuro6aH91dbXcbvd59bGxsYqNjQ31MACgw4QiKLpy0IRbW3NJIpsAdH5kk7GQzwTFxMQoLS1NJSUlgX0tLS0qKSlRRkZGqF8OAABD5BIA4Jva5cNSCwoKlJubq1GjRummm27SihUrVF9fr/vuu689Xg4AwoqrbfZHLgGINGSTsXZpgn74wx/q888/16JFi+T1ejVy5Eht3rz5vDelAkBXQNDYH7kEINKQTcbapQmSpPz8fOXn57fX0wMA0CbkEgCgVbs1QQAQKbjaBgCwG7LJGE0QAFhE0AAA7IZsMhb2zwkCAAAAgI7ETBAAWMTVNgCA3ZBNxmiCAMAiggYAYDdkkzGWwwEAAACIKMwEAYBFXG0DANgN2WSMJggALCJoAAB2QzYZYzkcAAAAgIjCTBAAWMTVNgCA3ZBNxmiCAMAiggYAYDdkkzGWwwEAAACIKMwEAYBFXG0DANgN2WSMmSAAsKg1aKxuAACESjiyqaysTFOmTJHH41FUVJQ2btwYONbU1KT58+dr2LBh6t27tzwej370ox/p2LFjQc9x8uRJzZgxQw6HQwkJCZo5c6ZOnz4dVLNv3z7ddtttiouLU0pKipYvX97m7w9NEAAAAADL6uvrNWLECBUVFZ137MyZM9qzZ48eeugh7dmzR3/6059UWVmp//f//l9Q3YwZM3TgwAEVFxdr06ZNKisr0+zZswPHfT6fJkyYoP79+6uiokJPPPGElixZoueee65NY2U5HABYxJIDAIDdhCObsrOzlZ2dfcFjTqdTxcXFQft+85vf6KabbtLRo0fVr18/ffjhh9q8ebN27dqlUaNGSZJWrVqlSZMm6cknn5TH49G6devU2NioP/zhD4qJidGQIUO0d+9ePfXUU0HN0qUwEwQAFoVjyUFhYaFGjx6t+Ph4JSYmaurUqaqsrAyqOXv2rPLy8tSnTx9dccUVysnJUXV1dVDN0aNHNXnyZPXq1UuJiYmaN2+ezp07Z/l7AgAIr1Bmk8/nC9oaGhpCMsa6ujpFRUUpISFBklReXq6EhIRAAyRJmZmZio6O1o4dOwI1Y8eOVUxMTKAmKytLlZWV+vLLL02/Nk0QAHRCpaWlysvL0/bt21VcXKympiZNmDBB9fX1gZoHHnhAr7/+ul555RWVlpbq2LFjmjZtWuB4c3OzJk+erMbGRm3btk3PP/+81q5dq0WLFoXjlAAANpWSkiKn0xnYCgsLLT/n2bNnNX/+fN11111yOBySJK/Xq8TExKC67t27y+Vyyev1BmqSkpKCaloft9aYwXI4ALAoHEsONm/eHPR47dq1SkxMVEVFhcaOHau6ujr9/ve/1/r16/Wd73xHkrRmzRoNGjRI27dv180336y33npLH3zwgd5++20lJSVp5MiRevjhhzV//nwtWbIk6CobAKBzCWU2VVVVBRoVSYqNjbX0vE1NTfrBD34gv9+vZ5991tJzXS5mggAgBEK1FO5ylxzU1dVJklwulySpoqJCTU1NyszMDNQMHDhQ/fr1U3l5uaSvlhQMGzYs6IpaVlaWfD6fDhw4YPl7AgAIr1Blk8PhCNqsNEGtDdAnn3yi4uLioObK7XarpqYmqP7cuXM6efKk3G53oOabS7tbH7fWmEETBAA2cjlLDlpaWjR37lzdeuutGjp0qKSvlgTExMQE1lm3SkpKCvmSAgAAzGhtgA4ePKi3335bffr0CTqekZGh2tpaVVRUBPZt2bJFLS0tSk9PD9SUlZWpqakpUFNcXKwBAwboyiuvND0WlsMBgEXhXnKQl5en/fv367333rM0BgBA1xGOpdqnT5/WoUOHAo+PHDmivXv3yuVyKTk5Wd///ve1Z88ebdq0Sc3NzYELbi6XSzExMRo0aJAmTpyoWbNmafXq1WpqalJ+fr6mT58uj8cjSbr77ru1dOlSzZw5U/Pnz9f+/fv161//Wk8//XSbxkoTBAAWhTJoWpcamJWfnx/4HIVrrrkmsN/tdquxsVG1tbVBs0HV1dVBSwp27twZ9HyXs6QAAGA/4WiCdu/erdtvvz3wuKCgQJKUm5urJUuW6LXXXpMkjRw5Mujr3nnnHY0bN06StG7dOuXn52v8+PGKjo5WTk6OVq5cGah1Op166623lJeXp7S0NPXt21eLFi1q0+2xJZogAOiU/H6/5syZow0bNmjr1q1KTU0NOp6WlqYePXqopKREOTk5kqTKykodPXpUGRkZkr5aUvDoo4+qpqYmcDee1vXZgwcP7tgTAgB0euPGjTNsnMw0VS6XS+vXrzesGT58uN599902j+/raIIAwKJwXG3Ly8vT+vXr9eqrryo+Pj6wpMDpdKpnz55yOp2aOXOmCgoK5HK55HA4NGfOHGVkZOjmm2+WJE2YMEGDBw/WPffco+XLl8vr9WrhwoXKy8uzfOcfAEB48UHexmiCAMCicARN6y1FW5cPtFqzZo3uvfdeSdLTTz8dWErQ0NCgrKwsPfPMM4Habt26adOmTbr//vuVkZGh3r17Kzc3V8uWLbN0LgCA8KMJMkYTBACdkJlgiouLU1FRkYqKii5a079/f73xxhuhHBoAALZHEwQAFnG1DQBgN2STMZogALCIoAEiS7du3cLyuq0fhnwpUVFRpurMvvfv+PHjpuq++bljF/PNzy8z0tzcbKruxIkTpuo+//xz06/d2ZFNxviwVAAAAAARhZkgALCIq20AALshm4zRBAGARQQNAMBuyCZjLIcDAAAAEFGYCQIAi7jaBgCwG7LJGE0QAFhE0AAA7IZsMsZyOAAAAAARhZkgALCIq20AALshm4zRBAGARQQNAMBuyCZjNEEIC7Oftu10Ott5JBeWn59vqq5Xr16m6gYMGGCqLi8vz1Tdk08+aarurrvuMlUnSWfPnjVV98tf/tJU3dKlS02/NoDI0KNHD1N1UVFRpup69uxpqq53796m6mJiYkzV9e3b11RddLS5dx306dPHVN23vvUtU3XXXHONqbq6ujpTdVVVVabq0tLSTNV9+9vfNlUnSbW1tabq/uu//stU3RtvvGGq7vPPPzdVh84r5O8JWrJkiaKiooK2gQMHhvplAMA2Wq+2Wd3QfsgmAJGGbDLWLjNBQ4YM0dtvv/1/L9KdCScAXRdLDjoHsglAJCGbjLVLAnTv3l1ut7s9nhoAgMtCNgEAWrXLLbIPHjwoj8ej6667TjNmzNDRo0cvWtvQ0CCfzxe0AUBnwpKDzoFsAhBJyCZjIW+C0tPTtXbtWm3evFnPPvusjhw5ottuu02nTp26YH1hYaGcTmdgS0lJCfWQAKBdETT2RzYBiDRkk7GQN0HZ2dn6p3/6Jw0fPlxZWVl64403VFtbqz/+8Y8XrF+wYIHq6uoCm9k7kAAAYBbZBAD4unZ/V2hCQoK+9a1v6dChQxc8Hhsbq9jY2PYeBgC0G9582vmQTQC6OrLJWLu8J+jrTp8+rcOHDys5Obm9XwoAwoblBp0L2QQgEpBNFxfyJuhnP/uZSktL9fHHH2vbtm2688471a1btzZ9aCMAAKFENgEAvi7ky+E+/fRT3XXXXTpx4oSuuuoqjRkzRtu3b9dVV10V6peCgX79+pmqM/vp2LfccoupujFjxpiqS0hIMFWXk5Njqs7uPv30U1N1K1euNFV35513mqq72Ju+L+Qvf/mLqbrS0lLTzxkpWHJgf2RT+4mPjzddO2TIEFN1ffr0MVVnNuuuu+66kD5famqqqTqHw2GqziyXy2Wqrrm52VRdZWWlqbqLLRv9pgEDBpiqa8v3xegujl/3xRdfmKqrr683/dqdHdlkLORN0EsvvRTqpwQAWyNo7I9sAhBpyCZj7f6eIAAAAACwk3a/OxwAdHVcbQMA2A3ZZIwmCAAsImgAAHZDNhljORwAAACAiMJMEABYxNU2AIDdkE3GaIIAwCKCBgBgN2STMZbDAQAAAIgozAQBgEVcbQMA2A3ZZIwmqBMZOXKk6dotW7aYqnM6nZc5GkhSS0uLqbqFCxeaqjt9+rSpunXr1pmqO378uKk6Sfryyy9N1Zn9hPFIQtCgK4qLizNVN3z4cNPPWVBQYKpuyJAhpuri4+NN1fXq1ctUXbdu3UL6un//+99N1TU3N5uq6927t6m6Tz/91FRdSUmJqTqzv/dLS0tN1Z06dcpUnWQ+x/72t7+Zqjtz5ozp1+7syCZjLIcDAAAAEFGYCQIAi7jaBgCwG7LJGE0QAFhE0AAA7IZsMsZyOAAAAAARhZkgALCIq20AALshm4wxEwQAFrUGjdUNAIBQCUc2lZWVacqUKfJ4PIqKitLGjRvPG9OiRYuUnJysnj17KjMzUwcPHgyqOXnypGbMmCGHw6GEhATNnDnzvLvn7tu3T7fddpvi4uKUkpKi5cuXt/n7QxMEAAAAwLL6+nqNGDFCRUVFFzy+fPlyrVy5UqtXr9aOHTvUu3dvZWVl6ezZs4GaGTNm6MCBAyouLtamTZtUVlam2bNnB477fD5NmDBB/fv3V0VFhZ544gktWbJEzz33XJvGynI4ALCIJQcAALsJRzZlZ2crOzv7os+1YsUKLVy4UHfccYck6YUXXlBSUpI2btyo6dOn68MPP9TmzZu1a9cujRo1SpK0atUqTZo0SU8++aQ8Ho/WrVunxsZG/eEPf1BMTIyGDBmivXv36qmnngpqli6FmSAAsIjlcAAAuwllNvl8vqCtoaGhzeM5cuSIvF6vMjMzA/ucTqfS09NVXl4uSSovL1dCQkKgAZKkzMxMRUdHa8eOHYGasWPHKiYmJlCTlZWlyspK0x/8LtEEAQAAADCQkpIip9MZ2AoLC9v8HF6vV5KUlJQUtD8pKSlwzOv1KjExMeh49+7d5XK5gmou9Bxffw0zWA7XiRw9etR07YkTJ0zVOZ3Oyx2OrbReHbiU2tpaU3W33367qbrGxkZTdf/xH/9hqg6dE8vh0BU1NTWZqvv6Wv5QufLKK03VtbS0mKpLSEiwMJrzmb3avHPnTlN1Zr/XycnJpupOnTplqq64uNhU3YEDB0zVmXXu3DnTtc3NzabqzOZxJAllNlVVVcnhcAT2x8bGWnpeO6AJAgCLaIIAAHYTymxyOBxBTdDlcLvdkqTq6uqghr66ulojR44M1NTU1AR93blz53Ty5MnA17vdblVXVwfVtD5urTGD5XAAAAAA2lVqaqrcbrdKSkoC+3w+n3bs2KGMjAxJUkZGhmpra1VRURGo2bJli1paWpSenh6oKSsrC5pBLS4u1oABA0zPIks0QQBgGTdGAADYTTiy6fTp09q7d6/27t0r6aubIezdu1dHjx5VVFSU5s6dq0ceeUSvvfaa3n//ff3oRz+Sx+PR1KlTJUmDBg3SxIkTNWvWLO3cuVN//vOflZ+fr+nTp8vj8UiS7r77bsXExGjmzJk6cOCAXn75Zf36179WQUFBm8bKcjgAsIjlcAAAuwlHNu3evTvofdWtjUlubq7Wrl2rBx98UPX19Zo9e7Zqa2s1ZswYbd68WXFxcYGvWbdunfLz8zV+/HhFR0crJydHK1euDBx3Op166623lJeXp7S0NPXt21eLFi1q0+2xJZogAAAAACEwbtw4w8YpKipKy5Yt07Jlyy5a43K5tH79esPXGT58uN59993LHqdEEwQAIcFMDgDAbsimi6MJAgCLWA4HALAbsskYN0YAAAAAEFGYCQIAi7jaBgCwG7LJGE1QJ3Ly5EnTtfPmzTNV973vfc9U3f/+7/+aqvv63TtCofUWi5fy3e9+11RdfX29qbohQ4aYqvvpT39qqg5dG0GDrqi5udlU3eHDh00/53/913+Zqtu/f7+pOpfLZapu/PjxpupSU1NN1X39c06M/O53vzNV9/HHH5uqGzx4sKk6s9+XTz75xFTdqVOnTNXBXsgmYyyHAwAAABBRmAkCAIu42gYAsBuyyRhNEABYRNAAAOyGbDLGcjgAAAAAEYWZIACwiKttAAC7IZuM0QQBgEUEDQDAbsgmYyyHAwAAABBRmAkCAIu42gYAsBuyyRhNEABYRNAAAOyGbDJGE9RFbdy40VTdli1bTNWZ/bToESNGmKqbOXOmqbonn3zSVF19fb2pOrMOHDhgqm727NkhfV0A6Gxqa2tN177xxhum6rZt22aqzul0mqqLj483Vdfc3Gyq7uOPPzZVt3PnTlN1X375ZUhf1+Vymao7fvy4qTqgK2rze4LKyso0ZcoUeTweRUVFnffHtt/v16JFi5ScnKyePXsqMzNTBw8eDNV4AcB2Wq+2Wd1wecglADgf2WSszU1QfX29RowYoaKiogseX758uVauXKnVq1drx44d6t27t7KysnT27FnLgwUAOwpH0FzqD/97771XUVFRQdvEiRODak6ePKkZM2bI4XAoISFBM2fO1OnTp61+OzocuQQA56MJMtbm5XDZ2dnKzs6+4DG/368VK1Zo4cKFuuOOOyRJL7zwgpKSkrRx40ZNnz7d2mgBAJL+7w//H//4x5o2bdoFayZOnKg1a9YEHsfGxgYdnzFjho4fP67i4mI1NTXpvvvu0+zZs7V+/fp2HXuokUsAgLYK6XuCjhw5Iq/Xq8zMzMA+p9Op9PR0lZeXXzBsGhoa1NDQEHjs8/lCOSQAaHfhePOp0R/+rWJjY+V2uy947MMPP9TmzZu1a9cujRo1SpK0atUqTZo0SU8++aQ8Hk+bxmNXl5NLEtkEoPPjxgjGQvo5QV6vV5KUlJQUtD8pKSlw7JsKCwvldDoDW0pKSiiHBADtLpRLDnw+X9D29T/E22rr1q1KTEzUgAEDdP/99+vEiROBY+Xl5UpISAg0QJKUmZmp6Oho7dix4/K/GTZzObkkkU0AOj+WwxkL+4elLliwQHV1dYGtqqoq3EMCgLBJSUkJ+uO7sLDwsp5n4sSJeuGFF1RSUqLHH39cpaWlys7ODtz9yuv1KjExMehrunfvLpfLZdgcRAqyCQC6tpAuh2tddlFdXa3k5OTA/urqao0cOfKCXxMbG3veOnUA6ExCueSgqqpKDocjsP9yfz9+fZnXsGHDNHz4cF1//fXaunWrxo8fb2msncnl5JJENgHo/FgOZyykM0Gpqalyu90qKSkJ7PP5fNqxY4cyMjJC+VIAYBuhXHLgcDiCtlD9IX7dddepb9++OnTokKSvmoOampqgmnPnzunkyZMXfR9RZ0QuAYhULIcz1uaZoNOnTwdCVPrqTad79+6Vy+VSv379NHfuXD3yyCO68cYblZqaqoceekgej0dTp04N5bgBAG3w6aef6sSJE4HZkIyMDNXW1qqiokJpaWmSvvrw5JaWFqWnp4dzqG1GLgEA2qrNTdDu3bt1++23Bx4XFBRIknJzc7V27Vo9+OCDqq+v1+zZs1VbW6sxY8Zo8+bNiouLC92oETKhvuNRXV1dSJ9v1qxZpupefvllU3UtLS1WhgNcUDiWHBj94e9yubR06VLl5OTI7Xbr8OHDevDBB3XDDTcoKytLkjRo0CBNnDhRs2bN0urVq9XU1KT8/HxNnz69090ZjlzqPMxmxKlTp0zVmf2spwMHDpiqM3sBwOxs6dVXX22q7ssvvzRVZ/ZGKcePHzdVh66N5XDG2twEjRs3zvAbEhUVpWXLlmnZsmWWBgYAnUU4gsboD/9nn31W+/bt0/PPP6/a2lp5PB5NmDBBDz/8cNDyunXr1ik/P1/jx49XdHS0cnJytHLlSkvnEQ7kEgCcjybIWEhvjAAA6BiX+sP/zTffvORzuFyuTvfBqAAAhAJNEACEQFe+WgYA6JzIpoujCQIAi1hyAACwG7LJWNg/LBUAAAAAOhIzQQBgEVfbAAB2QzYZowkCAIsIGgCA3ZBNxlgOBwAAACCiMBMEABZxtQ0AYDdkkzGaIITUkiVLTNWlpaWZqvvHf/xHU3WZmZmm6t566y1TdUBbEDRAaLW0tJiqq6mpMVW3bds2U3UDBw40VTd06FBTdWPGjDFV19jYaKruiy++MFV38uRJU3Xo2sgmYyyHAwAAABBRmAkCAIu42gYAsBuyyRhNEABYRNAAAOyGbDLGcjgAAAAAEYWZIACwiKttAAC7IZuMMRMEABa1Bo3VDQCAUOnobGpubtZDDz2k1NRU9ezZU9dff70efvjhoOfw+/1atGiRkpOT1bNnT2VmZurgwYNBz3Py5EnNmDFDDodDCQkJmjlzpk6fPh2y70srmiAAAAAAljz++ON69tln9Zvf/EYffvihHn/8cS1fvlyrVq0K1CxfvlwrV67U6tWrtWPHDvXu3VtZWVk6e/ZsoGbGjBk6cOCAiouLtWnTJpWVlWn27NkhHy/L4QDAIpYcAADspqOzadu2bbrjjjs0efJkSdK1116rF198UTt37gw814oVK7Rw4ULdcccdkqQXXnhBSUlJ2rhxo6ZPn64PP/xQmzdv1q5duzRq1ChJ0qpVqzRp0iQ9+eST8ng8ls7n65gJAgCLWA4HALCbUGaTz+cL2hoaGs57vVtuuUUlJSX629/+Jkn6y1/+ovfee0/Z2dmSpCNHjsjr9QZ9wL3T6VR6errKy8slSeXl5UpISAg0QJKUmZmp6Oho7dixI6TfH2aCEFL19fWm6mbNmmWqbs+ePabqfvvb35qqe+edd0zV7d6921RdUVGRqTr+wAWA0DP7u9Xs7/Tk5GRTdW6321TdjBkzTNWNHDnSVN1f/vIXU3UlJSWm6j766CNTdefOnTNVh64rJSUl6PHixYu1ZMmSoH0///nP5fP5NHDgQHXr1k3Nzc169NFHA/8feL1eSVJSUlLQ1yUlJQWOeb1eJSYmBh3v3r27XC5XoCZUaIIAwCKWwwEA7CaU2VRVVSWHwxHYHxsbe17tH//4R61bt07r16/XkCFDtHfvXs2dO1cej0e5ubmWxtEeaIIAwCKaIACA3YQymxwOR1ATdCHz5s3Tz3/+c02fPl2SNGzYMH3yyScqLCxUbm5uYAa1uro6aNa1uro6MBvqdrtVU1MT9Lznzp3TyZMnTc/AmsV7ggAAAABYcubMGUVHB7cW3bp1U0tLiyQpNTVVbrc7aLmmz+fTjh07lJGRIUnKyMhQbW2tKioqAjVbtmxRS0uL0tPTQzpeZoIAwCJmggAAdtPR2TRlyhQ9+uij6tevn4YMGaL//d//1VNPPaUf//jHkqSoqCjNnTtXjzzyiG688UalpqbqoYceksfj0dSpUyVJgwYN0sSJEzVr1iytXr1aTU1Nys/P1/Tp00N6ZziJJggALKMJAgDYTUdn06pVq/TQQw/pX/7lX1RTUyOPx6N//ud/1qJFiwI1Dz74oOrr6zV79mzV1tZqzJgx2rx5s+Li4gI169atU35+vsaPH6/o6Gjl5ORo5cqVls7jQmiCAAAAAFgSHx+vFStWaMWKFRetiYqK0rJly7Rs2bKL1rhcLq1fv74dRhiMJggALGImCABgN2STMZogALCIoAEA2A3ZZIy7wwEAAACIKMwEISwOHz5squ7ee+81VbdmzRpTdffcc09I63r37m2q7oUXXjBVd/z4cVN1sJ+ufLUM6OwaGhpM1b3zzjum6sx+XsmkSZNCWnfzzTebqjM7vtdff91U3f79+03VnT171lQdOg7ZdHHMBAEAAACIKMwEAYBFrLsGANgN2WSMJggALCJoAAB2QzYZYzkcAAAAgIjCTBAAWMTVNgCA3ZBNxmiCAMAiggYAYDdkkzGWwwEAAACIKMwEAYBFXG0DANgN2WSMJggALCJoAAB2QzYZowmCrW3YsMFU3cGDB03VPfXUU6bqxo8fb6ruscceM1XXv39/U3WPPvqoqbrPPvvMVB0AwLwTJ06Yqnv55ZdN1X3++eem6rKyskzV3XrrrabqfvCDH5iqczqdpupeffVVU3Xvvvuuqbpz586ZqgPaU5vfE1RWVqYpU6bI4/EoKipKGzduDDp+7733KioqKmibOHFiqMYLALbTerXN6obLQy4BwPnIJmNtngmqr6/XiBEj9OMf/1jTpk27YM3EiRO1Zs2awOPY2NjLHyEA2BxLDsKLXAKA85FNxtrcBGVnZys7O9uwJjY2Vm63+7IHBQCAWeQSAKCt2uUW2Vu3blViYqIGDBig+++/33CNbUNDg3w+X9AGAJ0JSw7sry25JJFNADo/sslYyJugiRMn6oUXXlBJSYkef/xxlZaWKjs7W83NzResLywslNPpDGwpKSmhHhIAtCuCxt7amksS2QSg8yObjIX87nDTp08P/HvYsGEaPny4rr/+em3duvWCd9xasGCBCgoKAo99Ph9hAwAImbbmkkQ2AUBX1y7L4b7uuuuuU9++fXXo0KELHo+NjZXD4QjaAKAz4Wpb53KpXJLIJgCdH9lkrN0/J+jTTz/ViRMnlJyc3N4vBQBhwR14OhdyCUAkIJuMtbkJOn36dNDVsyNHjmjv3r1yuVxyuVxaunSpcnJy5Ha7dfjwYT344IO64YYbTH8QGAAAbUEuAQDaqs1N0O7du3X77bcHHreumc7NzdWzzz6rffv26fnnn1dtba08Ho8mTJighx9+mM9kQLvav3+/qTqzn6I9ZcoUU3Vf/9wRI//8z/9squ7GG280Vffd737XVB06BlfbwotcQkf7/PPPTdW9/vrrpuoOHz5sqq6ystJUXX5+vqm6nJwcU3VxcXGm6qqrq03VffDBB6bqYA3ZZKzNTdC4ceMMvyFvvvmmpQEBQGdD0IQXuQQA5yObjLX7jREAAAAAwE7a/cYIANDVcbUNAGA3ZJMxmiAAsIigAQDYDdlkjOVwAAAAACIKM0EAYBFX2wAAdkM2GaMJAoAQ6MpBAQDonMimi2M5HAAAAICIwkwQAFjEkgMAgN2QTcZoghBRamtrTdX9x3/8h6m63/3ud6bqunc397/a2LFjTdWNGzfOVN3WrVtN1cEaggbAhTQ3N5uq+/TTT03VffLJJ6bqzGbdkCFDTNWlpaWZqrvqqqtM1fXq1ctUnSSdOXPGdC2CkU3GWA4HAAAAIKIwEwQAFnG1DQBgN2STMZogALCIoAEA2A3ZZIzlcAAAAAAiCjNBAGARV9sAAHZDNhljJggALGoNGqsbAAChEo5s+uyzz/T//X//n/r06aOePXtq2LBh2r17d9CYFi1apOTkZPXs2VOZmZk6ePBg0HOcPHlSM2bMkMPhUEJCgmbOnKnTp0+H5HvydTRBAAAAACz58ssvdeutt6pHjx76n//5H33wwQf61a9+pSuvvDJQs3z5cq1cuVKrV6/Wjh071Lt3b2VlZens2bOBmhkzZujAgQMqLi7Wpk2bVFZWptmzZ4d8vDRBAGBROK62lZWVacqUKfJ4PIqKitLGjRvPG5NdrrYBADpeR2fT448/rpSUFK1Zs0Y33XSTUlNTNWHCBF1//fWB8axYsUILFy7UHXfcoeHDh+uFF17QsWPHAhn24YcfavPmzfrd736n9PR0jRkzRqtWrdJLL72kY8eOhfT7QxMEABaFowmqr6/XiBEjVFRUdMHjdrraBgDoeKHMJp/PF7Q1NDSc93qvvfaaRo0apX/6p39SYmKivv3tb+u3v/1t4PiRI0fk9XqVmZkZ2Od0OpWenq7y8nJJUnl5uRISEjRq1KhATWZmpqKjo7Vjx46Qfn+4MQK6hOHDh5uq+/73v2+qbvTo0abquncP7f9CH3zwgam6srKykL4uOp/s7GxlZ2df8Ng3r7ZJ0gsvvKCkpCRt3LhR06dPD1xt27VrVyBsVq1apUmTJunJJ5+Ux+PpsHMB7CImJsZUndn/P4YOHWqq7tvf/rapuiFDhpiqu/baa03VXegP2Qs5efKkqbovv/zSVN2ZM2dM1cE+UlJSgh4vXrxYS5YsCdr30Ucf6dlnn1VBQYF+8YtfaNeuXfrXf/1XxcTEKDc3V16vV5KUlJQU9HVJSUmBY16vV4mJiUHHu3fvLpfLFagJFZogALAolHfg8fl8QftjY2MVGxvbpue61NW26dOnX/Jq25133mnhbAAA4RbKbKqqqpLD4Qjsv1AutbS0aNSoUXrsscckfdXc79+/X6tXr1Zubq6lcbQHlsMBgEWhXHKQkpIip9MZ2AoLC9s8HrtdbQMAdLxQZpPD4QjaLtQEJScna/DgwUH7Bg0apKNHj0qS3G63JKm6ujqoprq6OnDM7XarpqYm6Pi5c+d08uTJQE2o0AQBgI1UVVWprq4usC1YsCDcQwIA4JJuvfVWVVZWBu3729/+pv79+0uSUlNT5Xa7VVJSEjju8/m0Y8cOZWRkSJIyMjJUW1urioqKQM2WLVvU0tKi9PT0kI6X5XAAYFEolxy0XmWz4utX25KTkwP7q6urNXLkyEBNR11tAwB0vFBmkxkPPPCAbrnlFj322GP6wQ9+oJ07d+q5557Tc889J0mKiorS3Llz9cgjj+jGG29UamqqHnroIXk8Hk2dOlXSVzNHEydO1KxZs7R69Wo1NTUpPz9f06dPD/l7VZkJAgCLwnF3OCN2u9oGAOh4HZ1No0eP1oYNG/Tiiy9q6NChevjhh7VixQrNmDEjUPPggw9qzpw5mj17tkaPHq3Tp09r8+bNiouLC9SsW7dOAwcO1Pjx4zVp0iSNGTMm0EiFEjNBANAJnT59WocOHQo8PnLkiPbu3SuXy6V+/frZ6mobACAyfO9739P3vve9ix6PiorSsmXLtGzZsovWuFwurV+/vj2GF4QmCAAs6uglB5K0e/du3X777YHHBQUFkqTc3FytXbtWDz74oOrr6zV79mzV1tZqzJgxF7zalp+fr/Hjxys6Olo5OTlauXKlpfMAANhDOLKpM6EJAgCLwhE048aNM/waO11tAwB0PJogY7wnCAAAAEBEYSYIYTFgwABTdfn5+abqpk2bZqouXHe9am5uNlV3/PhxU3UtLS1WhoMQ42obEB7du5v7M+aqq64yVfcP//APpurGjh1rqu6WW24xVXfDDTeYqrviiitM1XXr1s1U3UcffWSq7i9/+Yupurq6OlN16BhkkzGaIACwiKABANgN2WSM5XAAAAAAIgozQQBgEVfbAAB2QzYZowkCgBDoykEBAOicyKaLYzkcAAAAgIjCTBAAWMSSAwCA3ZBNxmiCAMAiggYAYDdkkzGWwwEAAACIKMwEAYBFXG0DANgN2WSMJgimuN1uU3V33XWXqbr8/HxTdddee62punDZvXu3qbpHH33UVN1rr71mZTgIE4IGMKdbt26m6q644gpTdSNHjjRVd8stt5iqGzdunKm6ESNGmKrr06ePqbqmpiZTdZ9++qmpOrPZtHXrVlN1f/7zn03VffLJJ6bq0DHIJmNtWg5XWFio0aNHKz4+XomJiZo6daoqKyuDas6ePau8vDz16dNHV1xxhXJyclRdXR3SQQMA0IpsAgC0VZuaoNLSUuXl5Wn79u0qLi5WU1OTJkyYoPr6+kDNAw88oNdff12vvPKKSktLdezYMU2bNi3kAwcAu2i92mZ1w+UhmwDgfGSTsTYth9u8eXPQ47Vr1yoxMVEVFRUaO3as6urq9Pvf/17r16/Xd77zHUnSmjVrNGjQIG3fvl0333xz6EYOADbBkoPwIpsA4HxkkzFLd4erq6uTJLlcLklSRUWFmpqalJmZGagZOHCg+vXrp/Ly8gs+R0NDg3w+X9AGAMDlIpsAAJdy2U1QS0uL5s6dq1tvvVVDhw6VJHm9XsXExCghISGoNikpSV6v94LPU1hYKKfTGdhSUlIud0gAEBYsObAPsgkAvkI2GbvsJigvL0/79+/XSy+9ZGkACxYsUF1dXWCrqqqy9HwA0NEIGvsgmwDgK2STscu6RXZ+fr42bdqksrIyXXPNNYH9brdbjY2Nqq2tDbriVl1dfdFbLMfGxio2NvZyhgEAQADZBAAwq00zQX6/X/n5+dqwYYO2bNmi1NTUoONpaWnq0aOHSkpKAvsqKyt19OhRZWRkhGbEAGAzXG0LL7IJAM5HNhlr00xQXl6e1q9fr1dffVXx8fGBtdROp1M9e/aU0+nUzJkzVVBQIJfLJYfDoTlz5igjI4O77wDosrgDT3iRTQBwPrLJWJuaoGeffVbS+Z+ovGbNGt17772SpKefflrR0dHKyclRQ0ODsrKy9Mwzz4RksDAvKSnJVN3gwYNN1f3mN78xVTdw4EBTdeGyY8cOU3VPPPGEqbpXX33VVF1LS4upOgBtRza1j27dupmqa8tNI8xm07e//W1TdV+/45+R0aNHm6pLTk42VXfu3DlTdR999JGpuj179piq27p1q6k6s1l3+PBhU3WnTp0yVQd0Jm1qgsx0g3FxcSoqKlJRUdFlDwoAOhOutoUX2QQA5yObjF3WjREAAP+HoAEA2A3ZZMzSh6UCAAAAQGfDTBAAWMTVNgCA3ZBNxmiCAMAiggYAYDdkkzGWwwEAAACIKMwEAYBFXG0DANgN2WSMJggALCJoAAB2QzYZYzkcAAAAgIjCTBAAWMTVNgCA3ZBNxmiCbMDlcpmq+/d//3fTzzly5EhTddddd53p5wyHbdu2mar71a9+ZaruzTffNFX397//3VQdIBE0sIeePXuaqktKSjJVN2jQIFN148ePN1UnSWlpaabqrr76alN1/fv3N1XXo0cPU3VVVVWm6v785z+bqisrKzNVt337dlN1R44cMVVXV1dnqg5dG9lkjOVwAAAAACIKM0EAEAJd+WoZAKBzIpsujiYIACxiyQEAwG7IJmMshwMAAAAQUZgJAgCLuNoGALAbsskYM0EAYFFr0FjdAAAIlXBn0y9/+UtFRUVp7ty5gX1nz55VXl6e+vTpoyuuuEI5OTmqrq4O+rqjR49q8uTJ6tWrlxITEzVv3jydO3fussdxMTRBAAAAAEJm165d+vd//3cNHz48aP8DDzyg119/Xa+88opKS0t17NgxTZs2LXC8ublZkydPVmNjo7Zt26bnn39ea9eu1aJFi0I+RpogALAo3FfbAAD4pnBl0+nTpzVjxgz99re/1ZVXXhnYX1dXp9///vd66qmn9J3vfEdpaWlas2aNtm3bFvisrLfeeksffPCB/vM//1MjR45Udna2Hn74YRUVFamxsTFk3xuJJggALKMJAgDYTSizyefzBW0NDQ0Xfd28vDxNnjxZmZmZQfsrKirU1NQUtH/gwIHq16+fysvLJUnl5eUaNmxY0Ic6Z2Vlyefz6cCBA6H89nBjhMuRnp5uqm7evHmm6m666SZTdWY/QTuczpw5Y6pu5cqVpuoee+wxU3X19fWm6gCgs+nZs6eputtuu81U3ZQpU0zVfXMZy8UMHTrUVJ0kxcTEmKoz+gPr6+rq6kzVffDBB6bqysrKTNW9+eabpur2799vqs7n85mq42IJwiUlJSXo8eLFi7VkyZLz6l566SXt2bNHu3btOu+Y1+tVTEyMEhISgvYnJSXJ6/UGar7eALUebz0WSjRBAGARd+ABANhNKLOpqqpKDocjsD82Nva82qqqKv30pz9VcXGx4uLiLL1uR2A5HABYxHI4AIDdhDKbHA5H0HahJqiiokI1NTX6h3/4B3Xv3l3du3dXaWmpVq5cqe7duyspKUmNjY2qra0N+rrq6mq53W5JktvtPu9uca2PW2tChSYIAAAAgCXjx4/X+++/r7179wa2UaNGacaMGYF/9+jRQyUlJYGvqays1NGjR5WRkSFJysjI0Pvvv6+amppATXFxsRwOhwYPHhzS8bIcDgAsYjkcAMBuOjqb4uPjz3uPYO/evdWnT5/A/pkzZ6qgoEAul0sOh0Nz5sxRRkaGbr75ZknShAkTNHjwYN1zzz1avny5vF6vFi5cqLy8vAvOPllBEwQAFtEEAQDsxo7Z9PTTTys6Olo5OTlqaGhQVlaWnnnmmcDxbt26adOmTbr//vuVkZGh3r17Kzc3V8uWLQvpOCSaIAAAAADtYOvWrUGP4+LiVFRUpKKioot+Tf/+/fXGG2+088hoggDAMjtebQMARDayyRhNEABYRNAAAOyGbDLG3eEAAAAARBRmgi7DnXfeGdK69mD207E3bdpkqu7cuXOm6n71q1+ZqvvmPeKBzoyrbWhP8fHxpuquvvpqU3XXXnutqTqn02mq7pNPPjFVJ0mfffaZqbojR46Yqvvoo49M1e3fv99U3YEDB0zVmf3kev6/RjiRTcZoggDAIoIGAGA3ZJMxlsMBAAAAiCjMBAGARVxtAwDYDdlkjCYIACwiaAAAdkM2GWM5HAAAAICIwkwQAFjE1TYAgN2QTcZoggDAIoIGAGA3ZJMxlsMBAAAAiCjMBAFACHTlq2UAgM6JbLo4mqDL8POf/zykdQA6N5YcoD3V1NSYqvvv//5vU3U7duywMhxLGhoaTNWdOnUqpK/7xRdfmKpraWkJ6esC4UQ2GWvTcrjCwkKNHj1a8fHxSkxM1NSpU1VZWRlUM27cOEVFRQVtP/nJT0I6aAAAWpFNAIC2alMTVFpaqry8PG3fvl3FxcVqamrShAkTVF9fH1Q3a9YsHT9+PLAtX748pIMGADtpvdpmdWuLJUuWnPdH/cCBAwPHz549q7y8PPXp00dXXHGFcnJyVF1dHepTtwWyCQDOF45s6kzatBxu8+bNQY/Xrl2rxMREVVRUaOzYsYH9vXr1ktvtDs0IAcDmwrXkYMiQIXr77bcDj7t3/79f6Q888ID++7//W6+88oqcTqfy8/M1bdo0/fnPf7Y0TjsimwDgfCyHM2bp7nB1dXWSJJfLFbR/3bp16tu3r4YOHaoFCxbozJkzF32OhoYG+Xy+oA0AcGndu3eX2+0ObH379pX01e/m3//+93rqqaf0ne98R2lpaVqzZo22bdum7du3h3nU7Y9sAgBcymXfGKGlpUVz587VrbfeqqFDhwb233333erfv788Ho/27dun+fPnq7KyUn/6058u+DyFhYVaunTp5Q4DAMIulFfbvvnHdmxsrGJjYy/4NQcPHpTH41FcXJwyMjJUWFiofv36qaKiQk1NTcrMzAzUDhw4UP369VN5ebluvvlmS2O1M7IJAL7CTJCxy26C8vLytH//fr333ntB+2fPnh3497Bhw5ScnKzx48fr8OHDuv766897ngULFqigoCDw2OfzKSUl5XKHBQAdLpRB883ff4sXL9aSJUvOq09PT9fatWs1YMAAHT9+XEuXLtVtt92m/fv3y+v1KiYmRgkJCUFfk5SUJK/Xa2mcdkc2AcBXaIKMXVYTlJ+fr02bNqmsrEzXXHONYW16erok6dChQxcMGqOrnAAQaaqqquRwOAKPL/b7MTs7O/Dv4cOHKz09Xf3799cf//hH9ezZs93HaUdkEwDArDY1QX6/X3PmzNGGDRu0detWpaamXvJr9u7dK0lKTk6+rAECgN2F8mqbw+EIaoLMSkhI0Le+9S0dOnRI3/3ud9XY2Kja2tqg2aDq6uoueWMAsgkAzsdMkLE23RghLy9P//mf/6n169crPj5eXq9XXq9Xf//73yVJhw8f1sMPP6yKigp9/PHHeu211/SjH/1IY8eO1fDhw9vlBAAg3OxwG9LTp0/r8OHDSk5OVlpamnr06KGSkpLA8crKSh09elQZGRlWT9d2yCYAOJ8dssnO2jQT9Oyzz0r66kPnvm7NmjW69957FRMTo7ffflsrVqxQfX29UlJSlJOTo4ULF4ZswAAA6Wc/+5mmTJmi/v3769ixY1q8eLG6deumu+66S06nUzNnzlRBQYFcLpccDofmzJmjjIyMLnlTBLLpKzU1NSGtA4CurM3L4YykpKSotLTU0oAAoLMJx5KDTz/9VHfddZdOnDihq666SmPGjNH27dt11VVXSZKefvppRUdHKycnRw0NDcrKytIzzzxjaYx2RTYBwPlYDmfssu8OBwD4SjiC5qWXXjI8HhcXp6KiIhUVFVkZFgCgk6IJMmbpw1IBAAAAoLNhJggALOJqGwDAbsgmYzRBAGARQQMAsBuyyRjL4QAAAABEFGaCAMAirrYBAOyGbDJGEwQAFhE0AAC7IZuMsRwOAAAAQERhJggALOJqGwDAbsgmYzRBAGARQQMAsBuyyRjL4QAAAABEFGaCAMAirrYBAOyGbDJGEwQAFhE0AAC7IZuMsRwOAAAAQERhJggAQqArXy0DAHROZNPFMRMEABa1LjmwugEAECodnU2FhYUaPXq04uPjlZiYqKlTp6qysjKo5uzZs8rLy1OfPn10xRVXKCcnR9XV1UE1R48e1eTJk9WrVy8lJiZq3rx5OnfuXEi+J19HEwQAAADAktLSUuXl5Wn79u0qLi5WU1OTJkyYoPr6+kDNAw88oNdff12vvPKKSktLdezYMU2bNi1wvLm5WZMnT1ZjY6O2bdum559/XmvXrtWiRYtCPt4ov80uP9bV1SkhISHcwwAQAWpra+V0Oi/7630+n5xOp6699lpFR1u7ptTS0qKPP/5YdXV1cjgclp4LoUc2AegoXSWbPv/8cyUmJqq0tFRjx45VXV2drrrqKq1fv17f//73JUl//etfNWjQIJWXl+vmm2/W//zP/+h73/uejh07pqSkJEnS6tWrNX/+fH3++eeKiYmxdD5fZ7uZoFOnToV7CAAiRKh+37AcrusjmwB0FDtmk8/nC9oaGhou+fp1dXWSJJfLJUmqqKhQU1OTMjMzAzUDBw5Uv379VF5eLkkqLy/XsGHDAg2QJGVlZcnn8+nAgQMh+b60st2NETwej6qqqhQfH6+oqChJX33jU1JSVFVV1emvkHaVc+E87KernEtHnIff79epU6fk8Xja5fnR9XTlbOoq5yF1nXPhPOwn0rMpJSUl6PHixYu1ZMmSi9a3tLRo7ty5uvXWWzV06FBJktfrVUxMzHmz6klJSfJ6vYGarzdArcdbj4WS7Zqg6OhoXXPNNRc85nA4Ov3/RK26yrlwHvbTVc6lvc/DylKDb+KzGLq+SMimrnIeUtc5F87DfiI1m77Z/MXGxhp+XV5envbv36/33nvP0uu3J9s1QQDQ2dAEAQDsJpTZ1JbmLz8/X5s2bVJZWVnQxSO3263GxkbV1tYGzQZVV1fL7XYHanbu3Bn0fK13j2utCRXbvScIAAAAQOfi9/uVn5+vDRs2aMuWLUpNTQ06npaWph49eqikpCSwr7KyUkePHlVGRoYkKSMjQ++//75qamoCNcXFxXI4HBo8eHBIx9spZoJiY2O1ePHiS069dQZd5Vw4D/vpKufSGc+DmaDI1Bn/W72QrnIeUtc5F87DfjrjuXR0NuXl5Wn9+vV69dVXFR8fH3gPj9PpVM+ePeV0OjVz5kwVFBTI5XLJ4XBozpw5ysjI0M033yxJmjBhggYPHqx77rlHy5cvl9fr1cKFC5WXlxfy773tbpENAJ1F621Ir7766pDchvSzzz7jFtkAAEvClU2tN435pjVr1ujee++V9NWHpf7bv/2bXnzxRTU0NCgrK0vPPPNM0FK3Tz75RPfff7+2bt2q3r17Kzc3V7/85S/VvXto525oggDgMtEEAQDshmwyp1MshwMAO2M5HADAbsgmYzRBAGARQQMAsBuyyRh3hwMAAAAQUTpFE1RUVKRrr71WcXFxSk9PP+/+4Xa3ZMkSRUVFBW0DBw4M97BMKSsr05QpU+TxeBQVFaWNGzcGHff7/Vq0aJGSk5PVs2dPZWZm6uDBg+EZrIFLnce999573s9o4sSJ4RmsgcLCQo0ePVrx8fFKTEzU1KlTVVlZGVRz9uxZ5eXlqU+fPrriiiuUk5MTuMe+XZg5j3Hjxp33M/nJT34SphEba73aZnVD50I2hQ/ZZC9kE9nUGdm+CXr55ZdVUFCgxYsXa8+ePRoxYoSysrKC7h/eGQwZMkTHjx8PbHb+BN2vq6+v14gRI1RUVHTB48uXL9fKlSu1evVq7dixQ71791ZWVpbOnj3bwSM1dqnzkKSJEycG/YxefPHFDhyhOaWlpcrLy9P27dtVXFyspqYmTZgwQfX19YGaBx54QK+//rpeeeUVlZaW6tixY5o2bVoYR30+M+chSbNmzQr6mSxfvjxMIzZG0EQesim8yCZ7IZvIpk7Jb3M33XSTPy8vL/C4ubnZ7/F4/IWFhWEcVdssXrzYP2LEiHAPwzJJ/g0bNgQet7S0+N1ut/+JJ54I7KutrfXHxsb6X3zxxTCM0Jxvnoff7/fn5ub677jjjrCMx4qamhq/JH9paanf7//q+9+jRw//K6+8Eqj58MMP/ZL85eXl4RrmJX3zPPx+v/8f//Ef/T/96U/DNygT6urq/JL8SUlJ/uTkZEtbUlKSX5K/rq4u3KcFE8gm+yCb7IdsCi+yyRxbzwQ1NjaqoqJCmZmZgX3R0dHKzMxUeXl5GEfWdgcPHpTH49F1112nGTNm6OjRo+EekmVHjhyR1+sN+vk4nU6lp6d3up+PJG3dulWJiYkaMGCA7r//fp04cSLcQ7qkuro6SZLL5ZIkVVRUqKmpKehnMnDgQPXr18/WP5NvnkerdevWqW/fvho6dKgWLFigM2fOhGN4l+TnaltEIZvsjWwKP7LJHsgmY7a+O9wXX3yh5uZmJSUlBe1PSkrSX//61zCNqu3S09O1du1aDRgwQMePH9fSpUt12223af/+/YqPjw/38C5b6ycBX+jn03qss5g4caKmTZum1NRUHT58WL/4xS+UnZ2t8vJydevWLdzDu6CWlhbNnTtXt956q4YOHSrpq59JTEyMEhISgmrt/DO50HlI0t13363+/fvL4/Fo3759mj9/viorK/WnP/0pjKO9sFAERVcOmq6GbLI3sim8yCb7IJuM2boJ6iqys7MD/x4+fLjS09PVv39//fGPf9TMmTPDODK0mj59euDfw4YN0/Dhw3X99ddr69atGj9+fBhHdnF5eXnav39/p1nDfzEXO4/Zs2cH/j1s2DAlJydr/PjxOnz4sK6//vqOHibQ5ZBN9kc2hQ/Z1PXZejlc37591a1bt/PuHlJdXS232x2mUVmXkJCgb33rWzp06FC4h2JJ68+gq/18JOm6665T3759bfszys/P16ZNm/TOO+/ommuuCex3u91qbGxUbW1tUL1dfyYXO48LSU9PlyRb/kxYchBZyCZ7I5vCh2yyF7LJmK2boJiYGKWlpamkpCSwr6WlRSUlJcrIyAjjyKw5ffq0Dh8+rOTk5HAPxZLU1FS53e6gn4/P59OOHTs69c9Hkj799FOdOHHCdj8jv9+v/Px8bdiwQVu2bFFqamrQ8bS0NPXo0SPoZ1JZWamjR4/a6mdyqfO4kL1790qS7X4mEkETacgmeyObOh7ZRDZ1RrZfDldQUKDc3FyNGjVKN910k1asWKH6+nrdd9994R6aaT/72c80ZcoU9e/fX8eOHdPixYvVrVs33XXXXeEe2iWdPn066OrGkSNHtHfvXrlcLvXr109z587VI488ohtvvFGpqal66KGH5PF4NHXq1PAN+gKMzsPlcmnp0qXKycmR2+3W4cOH9eCDD+qGG25QVlZWGEd9vry8PK1fv16vvvqq4uPjA2upnU6nevbsKafTqZkzZ6qgoEAul0sOh0Nz5sxRRkaGbr755jCP/v9c6jwOHz6s9evXa9KkSerTp4/27dunBx54QGPHjtXw4cPDPHqAbAo3solsag9kU4TxdwKrVq3y9+vXzx8TE+O/6aab/Nu3bw/3kNrkhz/8oT85OdkfExPjv/rqq/0//OEP/YcOHQr3sEx55513/JLO23Jzc/1+/1e3In3ooYf8SUlJ/tjYWP/48eP9lZWV4R30BRidx5kzZ/wTJkzwX3XVVf4ePXr4+/fv7581a5bf6/WGe9jnudA5SPKvWbMmUPP3v//d/y//8i/+K6+80t+rVy//nXfe6T9+/Hj4Bn0BlzqPo0eP+seOHet3uVz+2NhY/w033OCfN2+e7W7R2XobUpfL5e/bt6+lzeVyddnbkHZVZFP4kE32QjbZ6/c22WROlN/fhee5AKAd+Xw+OZ1OXXnllYqOtra6uKWlRV9++aXq6urkcDhCNEIAQKQhm8yx9XuCAAAAACDUbP+eIADoDJhUBwDYDdl0cTRBAGBRKEKGoAIAhBLZZIzlcAAAAAAiCjNBAGARV9sAAHZDNhmjCQIAiwgaAIDdkE3GWA4HAAAAIKIwEwQAFnG1DQBgN2STMZogALCIoAEA2A3ZZIzlcAAAAAAiCjNBAGARV9sAAHZDNhmjCQIAiwgaAIDdkE3GWA4HAAAAIKIwEwQAFnG1DQBgN2STMZogALCIoAEA2A3ZZIzlcAAAAAAiCjNBAGARV9sAAHZDNhmjCQIAiwgaAIDdkE3GWA4HAAAAIKIwEwQAFnG1DQBgN2STMZogALCIoAEA2A3ZZIzlcAAAAAAiCjNBAGARV9sAAHZDNhmjCQIAiwgaAIDdkE3GWA4HAAAAIKLQBAGARX6/PyTb5SgqKtK1116ruLg4paena+fOnSE+OwBAZxSubOosuUQTBAAWhStoXn75ZRUUFGjx4sXas2ePRowYoaysLNXU1LTDWQIAOpNwZFNnyqUof1de7AcA7cjn88npdEqSoqKiLD1X66/iuro6ORwOU1+Tnp6u0aNH6ze/+Y0kqaWlRSkpKZozZ45+/vOfWxoPAKBzCmc2daZcYiYIAEKgo2eBGhsbVVFRoczMzMC+6OhoZWZmqry8PJSnBgDopDoymzpbLnF3OACwEZ/PF/Q4NjZWsbGx59V98cUXam5uVlJSUtD+pKQk/fWvf23XMQIAIouZbOpsucRMEABcppiYGLnd7pA93xVXXKGUlBQ5nc7AVlhYGLLnBwB0fWSTOcwEAcBliouL05EjR9TY2BiS5/P7/eet377QLJAk9e3bV926dVN1dXXQ/urq6pCGHwCgcwlXNnW2XKIJAgAL4uLiFBcX1+GvGxMTo7S0NJWUlGjq1KmSvnoDaklJifLz8zt8PAAA+whHNnW2XKIJAoBOqqCgQLm5uRo1apRuuukmrVixQvX19brvvvvCPTQAQATqTLlEEwQAndQPf/hDff7551q0aJG8Xq9GjhypzZs3n/emVAAAOkJnyiU+JwgAAABAROHucAAAAAAiCk0QAAAAgIhCEwQAAAAgotAEAQAAAIgoNEEAAAAAIgpNEAAAAICIQhMEAAAAIKLQBAEAAACIKDRBAAAAACIKTRAAAACAiEITBAAAACCi0AQBAAAAiCj/PyoQXUrLyw+7AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Create a 3x3 filter of your choice (a matrix of numbers) and save it as \"my_filter\".\n",
        "#\n",
        "# Then apply that filter to the example and save the output image as \"filtered_image\".\n",
        "#\n",
        "# Again, be sure to convert the example from unsigned int numbers to floating-point\n",
        "# numbers beforehand.\n",
        "\n",
        "# Create a filter of your choice\n",
        "my_filter = np.array([[0, 1, 0],\n",
        "                      [1, 2, 1],\n",
        "                      [0, 1, 0]])\n",
        "\n",
        "# Convert the example image to floating point numbers\n",
        "example = example.astype(np.float)\n",
        "\n",
        "# Apply the filter to the image\n",
        "filtered_image = scipy.signal.convolve2d(example, my_filter, mode='same')\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(example, cmap=\"gray\")\n",
        "plt.colorbar()\n",
        "           \n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(filtered_image, cmap=\"gray\")\n",
        "_ = plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ee626d21f249b0cc04f73acd1bfd0192",
          "grade": true,
          "grade_id": "cell-dab2b2dd97bd278f",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "wQLOdmJtnxEM"
      },
      "outputs": [],
      "source": [
        "assert blurred_image.shape == example.shape\n",
        "assert filtered_image.shape == example.shape\n",
        "assert my_filter.shape == (3,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYB8556wnxEM"
      },
      "source": [
        "### Feedforward NN versus CNN\n",
        "\n",
        "Let's build two models -- a dense feed-forward NN and a convolutional NN. We'll train each on the MNIST training set, test them on the test set, and compare performance results (accuracy). Because the test data set is balanced (approximately 1000 samples of each of the 10 digits), accuracy is a reasonably good metric so we'll just use that rather than F-score, for simplicity.\n",
        "\n",
        "In an effort have an apples-to-apples comparison, we'll use the same number of layers in both networks, and nearly the same number of total model parameters.\n",
        "\n",
        "__We'll start by creating a the feedforward network--one with two hidden layers plus an output layer.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "488b3397640cdf6f029e5df580de2466",
          "grade": false,
          "grade_id": "cell-dd8d2962acc9833c",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qO_rqssnxEM",
        "outputId": "862eb3b9-cb24-4b3a-aadd-2d31dc71ba04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten (Flatten)           (None, 784)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 25)                19625     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 25)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 25)                650       \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 25)                0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 10)                260       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,535\n",
            "Trainable params: 20,535\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Create a list of Keras layers, calling it \"ff_layers\", and use the\n",
        "# Keras Sequential class to create a feedforward NN model, as you\n",
        "# did in Activity 5a.\n",
        "#\n",
        "# Your model should have two hidden Dense layers and an output Dense layer.\n",
        "#\n",
        "# Use the \"relu\" activation function for the hidden layers. Other point-wise\n",
        "# non-linear activation function might work fairly well, but ReLU is the\n",
        "# most common because of its computational efficiency as well as its\n",
        "# effectiveness.\n",
        "#\n",
        "# Use 25 neurons in each of the hidden layers.\n",
        "#\n",
        "# This is a classification task with 10 output classes so select the output\n",
        "# layer's activation function accordingly. Review the reading material or lecture\n",
        "# slides from the first deep learning lecture, 5a, if you don't recall\n",
        "# what it should be or why.\n",
        "#\n",
        "# Since the input data (image) is a matrix, you'll need to additional layers\n",
        "# at the beginning of your \"ff_layers\" list.\n",
        "# 1. An Input() layer, that tells the model what shape the input samples\n",
        "#    will be, (28, 28, 1).\n",
        "# 2. A Flatten layer, to convert that image/matrix into a vector, which\n",
        "#    is the what the subsequent Dense layer expects.\n",
        "#\n",
        "# Also, add a Dropout layer after each hidden Dense layer, for regularization.\n",
        "#\n",
        "# Finally, save the resulting Sequential model as \"ff_model\"\n",
        "\n",
        "\n",
        "# Define the layers of the model\n",
        "ff_layers = [\n",
        "    Input(shape=(28, 28, 1)),\n",
        "    Flatten(),\n",
        "    Dense(25, activation=\"relu\"),\n",
        "    Dropout(0.2),\n",
        "    Dense(25, activation=\"relu\"),\n",
        "    Dropout(0.2),\n",
        "    Dense(10, activation=\"softmax\")\n",
        "]\n",
        "\n",
        "# Create the model\n",
        "ff_model = Sequential(ff_layers)\n",
        "\n",
        "ff_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeqE_MxNnxEM"
      },
      "source": [
        "Look at the output __above__, from `ff_model.summary()`, and take note of the __total number of model parameters__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a8bc339ed2ed5a9e26fd2303ea6f357d",
          "grade": true,
          "grade_id": "cell-b9330f9b40580d4b",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "M1UAkobFnxEM"
      },
      "outputs": [],
      "source": [
        "assert len(ff_layers) == 7\n",
        "target_type = [Flatten, Dense, Dropout, Dense, Dropout, Dense]\n",
        "for l, tt in zip(ff_model.layers, target_type):\n",
        "    assert isinstance(l, tt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWbPpjJinxEM"
      },
      "source": [
        "### Evaluating your test or validation set while training\n",
        "\n",
        "In previous notebooks we passed our training data to `Model.fit`, which trained for a specified number of epochs. Then we plotted the training set loss (and/or metrics) as a function of epochs, allowing us to see the dynamics of the model's convergence toward a solution.\n",
        "\n",
        "In this notebook we'll also pass our test data to `Model.fit`. The Model object is smart. It will use only the training data for fitting, but it will also compute the loss and metric functions for the test data, at the end of each training epoch. Afterwards, we can plot the loss and metrics for both the training data and for the test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 818
        },
        "id": "OT_hufCAnxEM",
        "outputId": "1329f83a-669d-4809-f0d2-6c045dd3332c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 2.5702 - accuracy: 0.1289 - val_loss: 2.0815 - val_accuracy: 0.2033\n",
            "Epoch 2/20\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 2.0729 - accuracy: 0.2023 - val_loss: 1.9378 - val_accuracy: 0.2459\n",
            "Epoch 3/20\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.9390 - accuracy: 0.2500 - val_loss: 1.7089 - val_accuracy: 0.3311\n",
            "Epoch 4/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 1.7666 - accuracy: 0.2991 - val_loss: 1.6073 - val_accuracy: 0.3446\n",
            "Epoch 5/20\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.6660 - accuracy: 0.3390 - val_loss: 1.3398 - val_accuracy: 0.4839\n",
            "Epoch 6/20\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.4959 - accuracy: 0.4400 - val_loss: 1.1770 - val_accuracy: 0.5922\n",
            "Epoch 7/20\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 1.3749 - accuracy: 0.4880 - val_loss: 1.0289 - val_accuracy: 0.6290\n",
            "Epoch 8/20\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.3002 - accuracy: 0.5340 - val_loss: 0.8815 - val_accuracy: 0.7312\n",
            "Epoch 9/20\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.2245 - accuracy: 0.5752 - val_loss: 0.8412 - val_accuracy: 0.7419\n",
            "Epoch 10/20\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 1.1920 - accuracy: 0.5877 - val_loss: 0.8119 - val_accuracy: 0.7520\n",
            "Epoch 11/20\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 1.1697 - accuracy: 0.5976 - val_loss: 0.8014 - val_accuracy: 0.7629\n",
            "Epoch 12/20\n",
            "1857/1875 [============================>.] - ETA: 0s - loss: 1.1542 - accuracy: 0.6047"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-023c7d671d48>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mff_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mff_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1727\u001b[0m                             \u001b[0msteps_per_execution\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1728\u001b[0m                         )\n\u001b[0;32m-> 1729\u001b[0;31m                     val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1730\u001b[0m                         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m                         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m   2070\u001b[0m                         ):\n\u001b[1;32m   2071\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2072\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2073\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2074\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    931\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mALLOW_DYNAMIC_VARIABLE_CREATION\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[0;32m--> 142\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m    143\u001b[0m     return concrete_function._call_flat(\n\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m     \"\"\"\n\u001b[1;32m    341\u001b[0m     args, kwargs, filtered_flat_args = (\n\u001b[0;32m--> 342\u001b[0;31m         self._function_spec.canonicalize_function_inputs(args, kwargs))\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/function_spec.py\u001b[0m in \u001b[0;36mcanonicalize_function_inputs\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    406\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_convert_variables_to_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind_function_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    409\u001b[0m     \u001b[0mfiltered_flat_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfilter_function_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/function_spec.py\u001b[0m in \u001b[0;36mcast_inputs\u001b[0;34m(args, kwargs, input_signature)\u001b[0m\n\u001b[1;32m    478\u001b[0m   \u001b[0;34m\"\"\"Casts args, kwargs to TF values based on an optional input_signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0minput_signature\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_numpy_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_inputs_to_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_signature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/function_spec.py\u001b[0m in \u001b[0;36mcast_numpy_inputs\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcast_numpy_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m   \u001b[0;34m\"\"\"Converts numpy array inputs to tensors.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomposite_tensor_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten_with_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m   \u001b[0;31m# Check for NumPy arrays in arguments and convert them to Tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/eager/polymorphic_function/composite_tensor_utils.py\u001b[0m in \u001b[0;36mflatten_with_variables\u001b[0;34m(inputs)\u001b[0m\n\u001b[1;32m     33\u001b[0m     if (isinstance(value, composite_tensor.CompositeTensor) and\n\u001b[1;32m     34\u001b[0m         not _pywrap_utils.IsResourceVariable(value)):\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mcomponents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_type_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_to_components\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflatten_with_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_type_spec\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    791\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_type_spec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 793\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mIteratorSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melement_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element_spec)\u001b[0m\n\u001b[1;32m    907\u001b[0m   \u001b[0m__slots__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"_element_spec\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_element_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melement_spec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "## Let's train the model and plot the training loss curve\n",
        "#\n",
        "# We'll train for 20 epochs. If you change model parameters/hyperparameters above,\n",
        "# and the model's loss curve hasn't flattened (approximately) after 20 epochs, you\n",
        "# can increase the number of training epochs.\n",
        "\n",
        "ff_model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "n_epochs = 20\n",
        "history = ff_model.fit(x_train, y_train, epochs=n_epochs, validation_data=(x_test, y_test))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(np.arange(1, n_epochs+1), history.history['loss'], label='Train set')\n",
        "plt.plot(np.arange(1, n_epochs+1), history.history['val_loss'], label='Test set')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(np.arange(1, n_epochs+1), history.history['accuracy'], label='Train set')\n",
        "plt.plot(np.arange(1, n_epochs+1), history.history['val_accuracy'], label='Test set')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "\n",
        "print(f\"\\nAccuracy on the final epoch of training was {100*history.history['accuracy'][-1]:0.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-045361b2e3f23134",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "id": "61_9Cy_RnxEN"
      },
      "outputs": [],
      "source": [
        "# Let's assess our FF model's performance on the test set\n",
        "ff_scores = ff_model.evaluate(x_test, y_test)\n",
        "\n",
        "print(f\"\\nThe fully-connected feedforward model achieves an accuracy of {ff_scores[1]*100:.2f}% on the test data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtEqMR49nxEN"
      },
      "source": [
        "### Result from the feedforward NN\n",
        "\n",
        "__You may find that the accuracy on the test set is higher than reported for the training set! But it's really not. Let us explain...__\n",
        "\n",
        "We used dropout during training, which helps with model regularization (and generalization) but reduces performance when in use (i.e., during training). __When we use the `evaluate()` method, the dropout layers are \"turned off.\"__ Similarly, `fit` does not apply dropout to the test/validation set, only to the training set. If we were to put our training set through the trained model, without dropout (that is, by using `evaluate`) we would get accuracy scores like those of the test data, and likely a little bit higher.\n",
        "\n",
        "What's a good accuracy for our model? Is 50% good? \n",
        "\n",
        "Something to consider is that we have 10 classes - does that change your answer? What would random guessing's accuracy be? \n",
        "\n",
        "Based on your answer to the above questions, is your simple model good? \n",
        "\n",
        "__Let's see if we can do better. In the next section, we'll build a convolutional neural network. You'll need to use [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [MaxPool2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D) layers from Keras.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ee25e18eda6ad104129ee745ae1a1401",
          "grade": false,
          "grade_id": "cell-960d34277a5b576e",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZsVikgBOnxEN",
        "outputId": "f3478ddb-1e92-40ee-ded8-1654858df42f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 28, 28, 16)        160       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 14, 14, 16)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 14, 14, 32)        4640      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 7, 7, 32)         0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 1568)              0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 10)                15690     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 20,490\n",
            "Trainable params: 20,490\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Now you'll build a CNN that has the same number of layers as the\n",
        "# feedforward NN, and a comparable number of model parameters.\n",
        "#\n",
        "# The CNN's convolutional and pooling layers will steadily reduce the\n",
        "# number of neurons/outputs in the vertical and horizontal dimensions\n",
        "# while increasing the number of channels (the \"depth\" dimension).\n",
        "#\n",
        "# Your CNN model should be defined by a list of layers, called \"cnn_layers\",\n",
        "# organized as such:\n",
        "# [Input(),\n",
        "#  Conv2D(),\n",
        "#  MaxPool2D(),\n",
        "#  Conv2D(),\n",
        "#  MaxPool2D(),\n",
        "#  Flatten(),\n",
        "#  Dense()]\n",
        "#\n",
        "# Create that list such that the first and second Conv2D layers have 16 and\n",
        "# 32 channels, respectively. Use a (3, 3)-shaped convolutional kernel\n",
        "# for both Conv2D layers, along with \"same\" padding and \"relu\" activation.\n",
        "#\n",
        "# You can use the defaults for the MaxPool2D layers (pooling over a 2x2\n",
        "# area, which thus halves the width and height dimensions).\n",
        "#\n",
        "# You'll then flatten the output of the second/last pooling layer, and\n",
        "# send that to a Dense output layer of 10 neurons. Be sure to use the\n",
        "# appropriate activation function for the output layer, as you did for\n",
        "# the feedforward NN.\n",
        "#\n",
        "# Dropout doesn't provide much regularization support in CNN models, as\n",
        "# the constraints imposed by architecture itself provides that. So we\n",
        "# won't use Dropout layers in the CNN.\n",
        "\n",
        "# Create your CNN model using Sequential, and save it as \"cnn_model\".\n",
        "\n",
        "# YOUR CODE HERE\n",
        "# create a CNN model\n",
        "cnn_layers = [\n",
        "    Input(shape=(28, 28, 1)),\n",
        "    Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
        "    MaxPool2D(),\n",
        "    Conv2D(32, (3, 3), padding='same', activation='relu'),\n",
        "    MaxPool2D(),\n",
        "    Flatten(),\n",
        "    Dense(10, activation='softmax')\n",
        "]\n",
        "cnn_model = Sequential(cnn_layers)\n",
        "\n",
        "cnn_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHHcSUJdnxEN"
      },
      "source": [
        "Look at the output __above__, from `cnn_model.summary()`, and take note of the __total number of model parameters__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "34fac23b1e3643b50ef90e676bb87d9c",
          "grade": true,
          "grade_id": "cell-44d2df9e1f2284ec",
          "locked": true,
          "points": 5,
          "schema_version": 3,
          "solution": false
        },
        "id": "YFvLJ702nxEN"
      },
      "outputs": [],
      "source": [
        "assert len(cnn_layers) == 7\n",
        "target_type = [Conv2D, MaxPool2D, Conv2D, MaxPool2D, Flatten, Dense]\n",
        "for l, tt in zip(cnn_model.layers, target_type):\n",
        "    assert isinstance(l, tt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Arld_QOvnxEN"
      },
      "outputs": [],
      "source": [
        "# Now we'll train the CNN model.\n",
        "#\n",
        "# Because the 2D architecture of the CNN constrains the model to learn patterns\n",
        "# that exist in 2D space (e.g., nearby pixels are more likely to exhibit some\n",
        "# connectedness/pattern than distant pixles) the model learns much more quickly\n",
        "# (that is, with fewer training epochs). So we'll only train for 3 epochs this time.\n",
        "# You can increase the number of epochs if you'd like.\n",
        "#\n",
        "# Also, the model expects the input samples to have a \"channel\" dimension, e.g.,\n",
        "# three channels (red/green/blue) for color images, 1 channel for grayscale images.\n",
        "# So we'll reshape our training images from (n_samples, 28, 28) to\n",
        "# (n_samples, 28, 28, 1).\n",
        "#\n",
        "# Note that we used 'sparse_categorical_crossentropy' as the loss function,\n",
        "# which is typical for classification tasks with a softmax output activation function.\n",
        "\n",
        "cnn_model.compile(optimizer=\"adam\", loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "n_epochs = 3\n",
        "history = cnn_model.fit(x_train.reshape(-1, 28, 28 ,1), y_train, epochs=n_epochs,\n",
        "                        validation_data=(x_test.reshape(-1, 28, 28 ,1), y_test))\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(np.arange(1, n_epochs+1), history.history['loss'], label='Train set')\n",
        "plt.plot(np.arange(1, n_epochs+1), history.history['val_loss'], label='Test set')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(np.arange(1, n_epochs+1), history.history['accuracy'], label='Train set')\n",
        "plt.plot(np.arange(1, n_epochs+1), history.history['val_accuracy'], label='Test set')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "print(f\"\\nAccuracy on the final epoch of training was {100*history.history['accuracy'][-1]:0.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "nbgrader": {
          "grade": false,
          "grade_id": "cell-a6157eb6365d54be",
          "locked": false,
          "schema_version": 3,
          "solution": false
        },
        "id": "JHyjezP4nxEO"
      },
      "outputs": [],
      "source": [
        "# Let's assess our CNN model's performance on the test set\n",
        "cnn_scores = cnn_model.evaluate(x_test.reshape(-1, 28, 28 ,1), y_test)\n",
        "\n",
        "print(f\"\\nThe CNN model achieves an accuracy of {cnn_scores[1]*100:.2f}% on the test data.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efn2NVwbnxEO"
      },
      "source": [
        "### Results from the CNN model\n",
        "\n",
        "If all went as planned, you saw notably higher accuracy from the CNN than from the feedforward network, __despite both networks having the same number of layers and nearly the same number of model parameters. It also converged much faster!__\n",
        "\n",
        "We didn't use dropout this time, so the \"artificial\" situation in which test set accuracy appears to be notably higher than training set accuracy should go away. However, even without dropout, the test set accuracy is often reported as being higher in *early* epochs, but this is for a different reason. The accuracy reported for a training set of a given epoch is the average of all batch accuracies *while the model was learning*, such that early batches did worse than later batches, within that epoch. The test set is not scored until after the training epoch is complete, so the model is \"better\" at that moment in time, and thus the test set accuracy score is higher.\n",
        "\n",
        "__Feel free to modify the FF and/or CNN networks__ -- increasing or decreasing number of neurons, numbers of layers, and types of activation functions. __Just be sure that your asserts/test cells pass after a fresh run of your notebook, before turning it in.__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "89d58854a6fbaf631d5783b3d8c8df27",
          "grade": true,
          "grade_id": "cell-279d9d45069805b8",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "T7fqcE_6nxEO"
      },
      "outputs": [],
      "source": [
        "assert cnn_scores[1] > 0.9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oLcNaEEnxEO"
      },
      "source": [
        "### Play in the sandbox\n",
        "\n",
        "Below you may want to explore the outputs of the two models for a variety of individual samples (images). With each run, the output predictions for all 10 classes are plotted, so you can __compare the \"sharpness\" of the predicted class probability distribution__ (using the term \"probability distribution\" loosely) of the CNN versus the FF model. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNTLnc5PnxEO"
      },
      "outputs": [],
      "source": [
        "# Change this value to test out some samples, and see what\n",
        "# them models' predictions look like.\n",
        "\n",
        "i = 373   # Pick any number from 0 to 9999\n",
        "\n",
        "new_example = x_test[i]\n",
        "ff_class_probabilities = ff_model.predict(new_example.reshape(-1, 28, 28))\n",
        "ff_class_prediction = np.argmax(ff_class_probabilities)\n",
        "\n",
        "cnn_class_probabilities = cnn_model.predict(new_example.reshape(-1, 28, 28, 1))\n",
        "cnn_class_prediction = np.argmax(cnn_class_probabilities)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(new_example, cmap=\"gray\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(ff_class_probabilities[0], '-bs', label='FF')\n",
        "plt.plot(ff_class_prediction, np.max(ff_class_probabilities[0]), 'bs', markersize=15)\n",
        "plt.plot(cnn_class_probabilities[0], '-ro', label='CNN')\n",
        "plt.plot(cnn_class_prediction, np.max(cnn_class_probabilities[0]), 'ro', markersize=15)\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Probability')\n",
        "plt.legend(loc='center right')\n",
        "plt.grid(True)\n",
        "\n",
        "print(f\"Feedforward model predicts class {ff_class_prediction}.\")\n",
        "print(f\"        CNN model predicts class {cnn_class_prediction}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "48f97c6b49b742a0a7ebbd9fc0415266",
          "grade": false,
          "grade_id": "cell-ad05bf85bb55dc8e",
          "locked": true,
          "schema_version": 3,
          "solution": false
        },
        "id": "RPmwxveCnxEO"
      },
      "source": [
        "## Feedback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ed936ab53a1391c5e6af8df699a1dbf5",
          "grade": false,
          "grade_id": "feedback",
          "locked": false,
          "schema_version": 3,
          "solution": true
        },
        "id": "VWyITRs4nxEO"
      },
      "outputs": [],
      "source": [
        "def feedback():\n",
        "    \"\"\"Provide feedback on the contents of this exercise\n",
        "    \n",
        "    Returns:\n",
        "        string\n",
        "    \"\"\"\n",
        "    # YOUR CODE HERE\n",
        "    raise NotImplementedError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f39f6185a54850c2f1f9b5b2a17b7543",
          "grade": true,
          "grade_id": "feedback-tests",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false
        },
        "id": "LqgtXOCrnxEP"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}